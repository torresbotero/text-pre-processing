{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from experiments_core import run_one_comb_experiment\n",
    "from experiments_core import norm_repeated_letters\n",
    "from experiments_core import stem_tokens\n",
    "from experiments_core import lemmatizer\n",
    "from experiments_core import save_excel_comb_results\n",
    "from experiments_core import RepeatReplacer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import xlsxwriter\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'experiments_core' from '/home/ctorres9/EAFIT/trabajogrado/experiments/experiments_core.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "importlib.reload(sys.modules[run_one_comb_experiment.__module__])\n",
    "importlib.reload(sys.modules[norm_repeated_letters.__module__])\n",
    "importlib.reload(sys.modules[stem_tokens.__module__])\n",
    "importlib.reload(sys.modules[lemmatizer.__module__])\n",
    "importlib.reload(sys.modules[save_excel_comb_results.__module__])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de archivos: 16\n",
      "                                                     text target\n",
      "count                                               10000  10000\n",
      "unique                                               9992      2\n",
      "top     Fernández Díaz nombra comisario honorario de l...   fake\n",
      "freq                                                    2   5000\n",
      "                                                text target\n",
      "0  Las lágrimas de un niño con autismo en un conc...   real\n",
      "1  Vender carne de mono por ternera https://t.co/...   real\n",
      "2  #Psicología Si buscas olvido en internet, verá...   real\n",
      "3  El cantante congoleño Papa Wemba fallece en pl...   real\n",
      "4  #Lomásvisto Vender carne de mono por ternera h...   real\n"
     ]
    }
   ],
   "source": [
    "files_path = 'data/satirical_real_news_tweets/'\n",
    "all_files = glob.glob(files_path + \"/*.csv\")\n",
    "print(\"Numero de archivos:\", len(all_files))\n",
    "original_news_df = pd.DataFrame()\n",
    "files_list = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file, index_col=None, header=0, sep='\\t')\n",
    "    files_list.append(df)\n",
    "original_news_df = pd.concat(files_list, ignore_index=True)\n",
    "original_news_df = original_news_df.drop(columns=['Unnamed: 0'])\n",
    "print(original_news_df.describe())\n",
    "print(original_news_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      "{} &                                                                                                                              text & target \\\\\n",
      "\\midrule\n",
      "8344 &                    10 frases de Paulo Coelho para convencer al casero de que baje el precio del alquiler: https://t.co/0FhBStsl3M &   fake \\\\\n",
      "9727 &                                                             Los 14 memes del 14 de febrero y San Valentín https://t.co/4nR3VvXhMb &   fake \\\\\n",
      "4425 &                        Greenpeace dejará de proteger a las ballenas para que aprendan a defenderse solas: https://t.co/uHguRI6NBt &   fake \\\\\n",
      "1530 &                         Cientificos descubren malformación genética que hace que a la gente le guste Maná https://t.co/VcyMkz7dIX &   fake \\\\\n",
      "8394 &  Trucos para saber si estás viviendo en el típico piso compartido de una “sitcom” https://t.co/d5eH07joNJ https://t.co/GpSxK1jIa4 &   fake \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(original_news_df[original_news_df.target == 'fake'].sample(5).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_content = original_news_df.text\n",
    "total_data_target = original_news_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(total_data_content, \n",
    "                                                    total_data_target, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reales entrenamiento: 3485\n",
      "Fakes entrenamiento: 3515\n",
      "Reales pruebas: 1515\n",
      "Fakes pruebas: 1485\n"
     ]
    }
   ],
   "source": [
    "print(\"Reales entrenamiento:\", len(y_train[y_train == 'real']))\n",
    "print(\"Fakes entrenamiento:\", len(y_train[y_train == 'fake']))\n",
    "print(\"Reales pruebas:\", len(y_test[y_test == 'real']))\n",
    "print(\"Fakes pruebas:\", len(y_test[y_test == 'fake']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_stopwords = stopwords.words('spanish')\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "replacer = RepeatReplacer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinaciones de experimentos para tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los siguientes son los resultados de las combinaciones entre los diferentes metodos de limpieza y ponderacion en la representacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_tasks = ['stop_words','stemming','lemmatization','urls','norm_letters','pruning10','pruning5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_tasks = []\n",
    "for r in range(1, len(pre_processing_tasks) + 1):\n",
    "    all_the_tasks = all_the_tasks + list(itertools.combinations(pre_processing_tasks, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_the_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "tasks_to_remove = []\n",
    "for comb in all_the_tasks:\n",
    "    if (('stemming' in comb) and ('lemmatization' in comb)) or (('pruning10' in comb) and ('pruning5' in comb)):\n",
    "        #all_the_tasks.remove(comb)\n",
    "        tasks_to_remove.append(comb)\n",
    "        count += 1\n",
    "print(count)\n",
    "print(len(tasks_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasks_to_remove:\n",
    "    all_the_tasks.remove(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stop_words',),\n",
       " ('stemming',),\n",
       " ('lemmatization',),\n",
       " ('urls',),\n",
       " ('norm_letters',),\n",
       " ('pruning10',),\n",
       " ('pruning5',),\n",
       " ('stop_words', 'stemming'),\n",
       " ('stop_words', 'lemmatization'),\n",
       " ('stop_words', 'urls'),\n",
       " ('stop_words', 'norm_letters'),\n",
       " ('stop_words', 'pruning10'),\n",
       " ('stop_words', 'pruning5'),\n",
       " ('stemming', 'urls'),\n",
       " ('stemming', 'norm_letters'),\n",
       " ('stemming', 'pruning10'),\n",
       " ('stemming', 'pruning5'),\n",
       " ('lemmatization', 'urls'),\n",
       " ('lemmatization', 'norm_letters'),\n",
       " ('lemmatization', 'pruning10'),\n",
       " ('lemmatization', 'pruning5'),\n",
       " ('urls', 'norm_letters'),\n",
       " ('urls', 'pruning10'),\n",
       " ('urls', 'pruning5'),\n",
       " ('norm_letters', 'pruning10'),\n",
       " ('norm_letters', 'pruning5'),\n",
       " ('stop_words', 'stemming', 'urls'),\n",
       " ('stop_words', 'stemming', 'norm_letters'),\n",
       " ('stop_words', 'stemming', 'pruning10'),\n",
       " ('stop_words', 'stemming', 'pruning5'),\n",
       " ('stop_words', 'lemmatization', 'urls'),\n",
       " ('stop_words', 'lemmatization', 'norm_letters'),\n",
       " ('stop_words', 'lemmatization', 'pruning10'),\n",
       " ('stop_words', 'lemmatization', 'pruning5'),\n",
       " ('stop_words', 'urls', 'norm_letters'),\n",
       " ('stop_words', 'urls', 'pruning10'),\n",
       " ('stop_words', 'urls', 'pruning5'),\n",
       " ('stop_words', 'norm_letters', 'pruning10'),\n",
       " ('stop_words', 'norm_letters', 'pruning5'),\n",
       " ('stemming', 'urls', 'norm_letters'),\n",
       " ('stemming', 'urls', 'pruning10'),\n",
       " ('stemming', 'urls', 'pruning5'),\n",
       " ('stemming', 'norm_letters', 'pruning10'),\n",
       " ('stemming', 'norm_letters', 'pruning5'),\n",
       " ('lemmatization', 'urls', 'norm_letters'),\n",
       " ('lemmatization', 'urls', 'pruning10'),\n",
       " ('lemmatization', 'urls', 'pruning5'),\n",
       " ('lemmatization', 'norm_letters', 'pruning10'),\n",
       " ('lemmatization', 'norm_letters', 'pruning5'),\n",
       " ('urls', 'norm_letters', 'pruning10'),\n",
       " ('urls', 'norm_letters', 'pruning5'),\n",
       " ('stop_words', 'stemming', 'urls', 'norm_letters'),\n",
       " ('stop_words', 'stemming', 'urls', 'pruning10'),\n",
       " ('stop_words', 'stemming', 'urls', 'pruning5'),\n",
       " ('stop_words', 'stemming', 'norm_letters', 'pruning10'),\n",
       " ('stop_words', 'stemming', 'norm_letters', 'pruning5'),\n",
       " ('stop_words', 'lemmatization', 'urls', 'norm_letters'),\n",
       " ('stop_words', 'lemmatization', 'urls', 'pruning10'),\n",
       " ('stop_words', 'lemmatization', 'urls', 'pruning5'),\n",
       " ('stop_words', 'lemmatization', 'norm_letters', 'pruning10'),\n",
       " ('stop_words', 'lemmatization', 'norm_letters', 'pruning5'),\n",
       " ('stop_words', 'urls', 'norm_letters', 'pruning10'),\n",
       " ('stop_words', 'urls', 'norm_letters', 'pruning5'),\n",
       " ('stemming', 'urls', 'norm_letters', 'pruning10'),\n",
       " ('stemming', 'urls', 'norm_letters', 'pruning5'),\n",
       " ('lemmatization', 'urls', 'norm_letters', 'pruning10'),\n",
       " ('lemmatization', 'urls', 'norm_letters', 'pruning5'),\n",
       " ('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning10'),\n",
       " ('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning5'),\n",
       " ('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning10'),\n",
       " ('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning5')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_the_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_test = [('stop_words'),('stemming'), ('lemmatization'), ('urls'), ('norm_letters'), ('pruning10'), ('pruning5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_types_weighting_types = [{'clf_name': 'Bayesiano', 'clf_type': MultinomialNB(), \n",
    "                              'weighting_type': 'TF', 'optimal_parameters': {'alpha': [1]}, 'random_state': 60},\n",
    "                             {'clf_name': 'Bayesiano', 'clf_type': MultinomialNB(), \n",
    "                              'weighting_type': 'TF-IDF', 'optimal_parameters': {'alpha': [0.1]}, 'random_state': 40},\n",
    "                             {'clf_name': 'Bayesiano', 'clf_type': MultinomialNB(), \n",
    "                              'weighting_type': 'Binario', 'optimal_parameters': {'alpha': [1]}, 'random_state': 60},\n",
    "                             \n",
    "                             {'clf_name': 'SVM_radial', 'clf_type': SVC(), \n",
    "                              'weighting_type': 'TF', 'optimal_parameters': {'kernel': ['rbf'], 'gamma': [0.001], 'C': [100]}, 'random_state': 90},\n",
    "                             {'clf_name': 'SVM_radial', 'clf_type': SVC(), \n",
    "                              'weighting_type': 'TF-IDF', 'optimal_parameters': {'kernel': ['rbf'], 'gamma': [0.001], 'C': [1000]}, 'random_state': 20},\n",
    "                             {'clf_name': 'SVM_radial', 'clf_type': SVC(), \n",
    "                              'weighting_type': 'Binario', 'optimal_parameters': {'kernel': ['rbf'], 'gamma': [0.001], 'C': [100]}, 'random_state': 90},\n",
    "                            \n",
    "                             {'clf_name': 'SVM_lineal', 'clf_type': LinearSVC(), \n",
    "                              'weighting_type': 'TF', 'optimal_parameters': {'C': [0.1]}, 'random_state': 90},\n",
    "                             {'clf_name': 'SVM_lineal', 'clf_type': LinearSVC(), \n",
    "                              'weighting_type': 'TF-IDF', 'optimal_parameters': {'C': [100]}, 'random_state': 20},\n",
    "                             {'clf_name': 'SVM_lineal', 'clf_type': LinearSVC(), \n",
    "                              'weighting_type': 'Binario', 'optimal_parameters': {'C': [0.1]}, 'random_state': 100},\n",
    "                            \n",
    "                             {'clf_name': 'Reg_Log', 'clf_type': LogisticRegression(), \n",
    "                              'weighting_type': 'TF', 'optimal_parameters': {'C': [1000]}, 'random_state': 90},\n",
    "                             {'clf_name': 'Reg_Log', 'clf_type': LogisticRegression(), \n",
    "                              'weighting_type': 'TF-IDF', 'optimal_parameters': {'C': [1000]}, 'random_state': 60},\n",
    "                             {'clf_name': 'Reg_Log', 'clf_type': LogisticRegression(), \n",
    "                              'weighting_type': 'Binario', 'optimal_parameters': {'C': [1000]}, 'random_state': 100}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'clf_name': 'Bayesiano',\n",
       "  'clf_type': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "  'weighting_type': 'TF',\n",
       "  'optimal_parameters': {'alpha': [1]},\n",
       "  'random_state': 60},\n",
       " {'clf_name': 'Bayesiano',\n",
       "  'clf_type': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "  'weighting_type': 'TF-IDF',\n",
       "  'optimal_parameters': {'alpha': [0.1]},\n",
       "  'random_state': 40},\n",
       " {'clf_name': 'Bayesiano',\n",
       "  'clf_type': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "  'weighting_type': 'Binario',\n",
       "  'optimal_parameters': {'alpha': [1]},\n",
       "  'random_state': 60},\n",
       " {'clf_name': 'SVM_radial',\n",
       "  'clf_type': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "  'weighting_type': 'TF',\n",
       "  'optimal_parameters': {'kernel': ['rbf'], 'gamma': [0.001], 'C': [100]},\n",
       "  'random_state': 90},\n",
       " {'clf_name': 'SVM_radial',\n",
       "  'clf_type': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "  'weighting_type': 'TF-IDF',\n",
       "  'optimal_parameters': {'kernel': ['rbf'], 'gamma': [0.001], 'C': [1000]},\n",
       "  'random_state': 20},\n",
       " {'clf_name': 'SVM_radial',\n",
       "  'clf_type': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False),\n",
       "  'weighting_type': 'Binario',\n",
       "  'optimal_parameters': {'kernel': ['rbf'], 'gamma': [0.001], 'C': [100]},\n",
       "  'random_state': 90},\n",
       " {'clf_name': 'SVM_lineal',\n",
       "  'clf_type': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "       intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "       multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "       verbose=0),\n",
       "  'weighting_type': 'TF',\n",
       "  'optimal_parameters': {'C': [0.1]},\n",
       "  'random_state': 90},\n",
       " {'clf_name': 'SVM_lineal',\n",
       "  'clf_type': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "       intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "       multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "       verbose=0),\n",
       "  'weighting_type': 'TF-IDF',\n",
       "  'optimal_parameters': {'C': [100]},\n",
       "  'random_state': 20},\n",
       " {'clf_name': 'SVM_lineal',\n",
       "  'clf_type': LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "       intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "       multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "       verbose=0),\n",
       "  'weighting_type': 'Binario',\n",
       "  'optimal_parameters': {'C': [0.1]},\n",
       "  'random_state': 100},\n",
       " {'clf_name': 'Reg_Log',\n",
       "  'clf_type': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  'weighting_type': 'TF',\n",
       "  'optimal_parameters': {'C': [1000]},\n",
       "  'random_state': 90},\n",
       " {'clf_name': 'Reg_Log',\n",
       "  'clf_type': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  'weighting_type': 'TF-IDF',\n",
       "  'optimal_parameters': {'C': [1000]},\n",
       "  'random_state': 60},\n",
       " {'clf_name': 'Reg_Log',\n",
       "  'clf_type': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  'weighting_type': 'Binario',\n",
       "  'optimal_parameters': {'C': [1000]},\n",
       "  'random_state': 100}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_types_weighting_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesiano_TF('stop_words',)\n",
      "train time: 9.904s\n",
      "Bayesiano_TF('stemming',)\n",
      "train time: 9.909s\n",
      "Bayesiano_TF('lemmatization',)\n",
      "train time: 10.341s\n",
      "Bayesiano_TF('urls',)\n",
      "train time: 9.970s\n",
      "Bayesiano_TF('norm_letters',)\n",
      "train time: 10.010s\n",
      "Bayesiano_TF('pruning10',)\n",
      "train time: 9.707s\n",
      "Bayesiano_TF('pruning5',)\n",
      "train time: 9.683s\n",
      "Bayesiano_TF('stop_words', 'stemming')\n",
      "train time: 9.978s\n",
      "Bayesiano_TF('stop_words', 'lemmatization')\n",
      "train time: 10.086s\n",
      "Bayesiano_TF('stop_words', 'urls')\n",
      "train time: 10.048s\n",
      "Bayesiano_TF('stop_words', 'norm_letters')\n",
      "train time: 9.861s\n",
      "Bayesiano_TF('stop_words', 'pruning10')\n",
      "train time: 10.036s\n",
      "Bayesiano_TF('stop_words', 'pruning5')\n",
      "train time: 10.276s\n",
      "Bayesiano_TF('norm_letters', 'pruning10')\n",
      "train time: 10.080s\n",
      "Bayesiano_TF('norm_letters', 'pruning5')\n",
      "train time: 10.241s\n",
      "Bayesiano_TF('stop_words', 'stemming', 'urls')\n",
      "train time: 10.438s\n",
      "Bayesiano_TF('stop_words', 'stemming', 'norm_letters')\n",
      "train time: 10.274s\n",
      "Bayesiano_TF('stop_words', 'stemming', 'pruning10')\n",
      "train time: 10.457s\n",
      "Bayesiano_TF('stop_words', 'stemming', 'pruning5')\n",
      "train time: 10.402s\n",
      "Bayesiano_TF('stop_words', 'lemmatization', 'urls')\n",
      "train time: 10.639s\n",
      "Bayesiano_TF('stop_words', 'lemmatization', 'norm_letters')\n",
      "train time: 10.723s\n",
      "Bayesiano_TF('stop_words', 'lemmatization', 'pruning10')\n",
      "train time: 10.942s\n",
      "Bayesiano_TF('stop_words', 'lemmatization', 'pruning5')\n",
      "train time: 10.517s\n",
      "Bayesiano_TF('stop_words', 'urls', 'norm_letters')\n",
      "train time: 10.550s\n",
      "Bayesiano_TF('stop_words', 'urls', 'pruning10')\n",
      "train time: 10.570s\n",
      "Bayesiano_TF('stop_words', 'urls', 'pruning5')\n",
      "train time: 10.655s\n",
      "Bayesiano_TF('stop_words', 'norm_letters', 'pruning10')\n",
      "train time: 11.101s\n",
      "Bayesiano_TF('stop_words', 'norm_letters', 'pruning5')\n",
      "train time: 11.018s\n",
      "Bayesiano_TF('stemming', 'urls', 'norm_letters')\n",
      "train time: 10.998s\n",
      "Bayesiano_TF('stemming', 'urls', 'pruning10')\n",
      "train time: 10.896s\n",
      "Bayesiano_TF('stemming', 'urls', 'pruning5')\n",
      "train time: 11.013s\n",
      "Bayesiano_TF('stemming', 'norm_letters', 'pruning10')\n",
      "train time: 11.092s\n",
      "Bayesiano_TF('stemming', 'norm_letters', 'pruning5')\n",
      "train time: 11.261s\n",
      "Bayesiano_TF('lemmatization', 'urls', 'norm_letters')\n",
      "train time: 11.179s\n",
      "Bayesiano_TF('lemmatization', 'urls', 'pruning10')\n",
      "train time: 11.213s\n",
      "Bayesiano_TF('lemmatization', 'urls', 'pruning5')\n",
      "train time: 11.149s\n",
      "Bayesiano_TF('lemmatization', 'norm_letters', 'pruning10')\n",
      "train time: 11.097s\n",
      "Bayesiano_TF('lemmatization', 'norm_letters', 'pruning5')\n",
      "train time: 11.345s\n",
      "Bayesiano_TF('urls', 'norm_letters', 'pruning10')\n",
      "train time: 11.465s\n",
      "Bayesiano_TF('urls', 'norm_letters', 'pruning5')\n",
      "train time: 11.665s\n",
      "Bayesiano_TF('stop_words', 'stemming', 'urls', 'norm_letters')\n",
      "train time: 11.556s\n",
      "Bayesiano_TF('stop_words', 'stemming', 'urls', 'pruning10')\n",
      "train time: 11.627s\n",
      "Bayesiano_TF('stop_words', 'stemming', 'urls', 'pruning5')\n",
      "train time: 11.332s\n",
      "Bayesiano_TF('stop_words', 'stemming', 'norm_letters', 'pruning10')\n",
      "train time: 11.270s\n",
      "Bayesiano_TF('stop_words', 'stemming', 'norm_letters', 'pruning5')\n",
      "train time: 11.629s\n",
      "Bayesiano_TF('stop_words', 'lemmatization', 'urls', 'norm_letters')\n",
      "train time: 11.356s\n",
      "Bayesiano_TF('stop_words', 'lemmatization', 'urls', 'pruning10')\n",
      "train time: 11.500s\n",
      "Bayesiano_TF('stop_words', 'lemmatization', 'urls', 'pruning5')\n",
      "train time: 11.580s\n",
      "Bayesiano_TF('stop_words', 'lemmatization', 'norm_letters', 'pruning10')\n",
      "train time: 11.688s\n",
      "Bayesiano_TF('stop_words', 'lemmatization', 'norm_letters', 'pruning5')\n",
      "train time: 11.860s\n",
      "Bayesiano_TF('stop_words', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 11.659s\n",
      "Bayesiano_TF('stop_words', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 11.497s\n",
      "Bayesiano_TF('stemming', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 11.725s\n",
      "Bayesiano_TF('stemming', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 11.875s\n",
      "Bayesiano_TF('lemmatization', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 11.802s\n",
      "Bayesiano_TF('lemmatization', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 11.787s\n",
      "Bayesiano_TF('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 11.748s\n",
      "Bayesiano_TF('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 11.937s\n",
      "Bayesiano_TF('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 12.165s\n",
      "Bayesiano_TF('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 12.046s\n",
      "Bayesiano_TF-IDF('stop_words',)\n",
      "train time: 12.319s\n",
      "Bayesiano_TF-IDF('stemming',)\n",
      "train time: 12.190s\n",
      "Bayesiano_TF-IDF('lemmatization',)\n",
      "train time: 12.390s\n",
      "Bayesiano_TF-IDF('urls',)\n",
      "train time: 12.193s\n",
      "Bayesiano_TF-IDF('norm_letters',)\n",
      "train time: 12.438s\n",
      "Bayesiano_TF-IDF('pruning10',)\n",
      "train time: 12.718s\n",
      "Bayesiano_TF-IDF('pruning5',)\n",
      "train time: 12.600s\n",
      "Bayesiano_TF-IDF('stop_words', 'stemming')\n",
      "train time: 12.697s\n",
      "Bayesiano_TF-IDF('stop_words', 'lemmatization')\n",
      "train time: 12.717s\n",
      "Bayesiano_TF-IDF('stop_words', 'urls')\n",
      "train time: 12.588s\n",
      "Bayesiano_TF-IDF('stop_words', 'norm_letters')\n",
      "train time: 12.968s\n",
      "Bayesiano_TF-IDF('stop_words', 'pruning10')\n",
      "train time: 12.784s\n",
      "Bayesiano_TF-IDF('stop_words', 'pruning5')\n",
      "train time: 12.881s\n",
      "Bayesiano_TF-IDF('stemming', 'urls')\n",
      "train time: 12.780s\n",
      "Bayesiano_TF-IDF('stemming', 'norm_letters')\n",
      "train time: 12.815s\n",
      "Bayesiano_TF-IDF('stemming', 'pruning10')\n",
      "train time: 13.137s\n",
      "Bayesiano_TF-IDF('stemming', 'pruning5')\n",
      "train time: 13.200s\n",
      "Bayesiano_TF-IDF('lemmatization', 'urls')\n",
      "train time: 13.022s\n",
      "Bayesiano_TF-IDF('lemmatization', 'norm_letters')\n",
      "train time: 13.364s\n",
      "Bayesiano_TF-IDF('lemmatization', 'pruning10')\n",
      "train time: 13.163s\n",
      "Bayesiano_TF-IDF('lemmatization', 'pruning5')\n",
      "train time: 13.359s\n",
      "Bayesiano_TF-IDF('urls', 'norm_letters')\n",
      "train time: 13.377s\n",
      "Bayesiano_TF-IDF('urls', 'pruning10')\n",
      "train time: 13.316s\n",
      "Bayesiano_TF-IDF('urls', 'pruning5')\n",
      "train time: 13.413s\n",
      "Bayesiano_TF-IDF('norm_letters', 'pruning10')\n",
      "train time: 13.591s\n",
      "Bayesiano_TF-IDF('norm_letters', 'pruning5')\n",
      "train time: 13.764s\n",
      "Bayesiano_TF-IDF('stop_words', 'stemming', 'urls')\n",
      "train time: 13.408s\n",
      "Bayesiano_TF-IDF('stop_words', 'stemming', 'norm_letters')\n",
      "train time: 13.605s\n",
      "Bayesiano_TF-IDF('stop_words', 'stemming', 'pruning10')\n",
      "train time: 13.645s\n",
      "Bayesiano_TF-IDF('stop_words', 'stemming', 'pruning5')\n",
      "train time: 13.721s\n",
      "Bayesiano_TF-IDF('stop_words', 'lemmatization', 'urls')\n",
      "train time: 13.520s\n",
      "Bayesiano_TF-IDF('stop_words', 'lemmatization', 'norm_letters')\n",
      "train time: 13.940s\n",
      "Bayesiano_TF-IDF('stop_words', 'lemmatization', 'pruning10')\n",
      "train time: 13.948s\n",
      "Bayesiano_TF-IDF('stop_words', 'lemmatization', 'pruning5')\n",
      "train time: 13.640s\n",
      "Bayesiano_TF-IDF('stop_words', 'urls', 'norm_letters')\n",
      "train time: 13.858s\n",
      "Bayesiano_TF-IDF('stop_words', 'urls', 'pruning10')\n",
      "train time: 13.770s\n",
      "Bayesiano_TF-IDF('stop_words', 'urls', 'pruning5')\n",
      "train time: 13.937s\n",
      "Bayesiano_TF-IDF('stop_words', 'norm_letters', 'pruning10')\n",
      "train time: 14.177s\n",
      "Bayesiano_TF-IDF('stop_words', 'norm_letters', 'pruning5')\n",
      "train time: 14.314s\n",
      "Bayesiano_TF-IDF('stemming', 'urls', 'norm_letters')\n",
      "train time: 14.217s\n",
      "Bayesiano_TF-IDF('stemming', 'urls', 'pruning10')\n",
      "train time: 14.432s\n",
      "Bayesiano_TF-IDF('stemming', 'urls', 'pruning5')\n",
      "train time: 14.348s\n",
      "Bayesiano_TF-IDF('stemming', 'norm_letters', 'pruning10')\n",
      "train time: 14.544s\n",
      "Bayesiano_TF-IDF('stemming', 'norm_letters', 'pruning5')\n",
      "train time: 14.425s\n",
      "Bayesiano_TF-IDF('lemmatization', 'urls', 'norm_letters')\n",
      "train time: 14.309s\n",
      "Bayesiano_TF-IDF('lemmatization', 'urls', 'pruning10')\n",
      "train time: 14.248s\n",
      "Bayesiano_TF-IDF('lemmatization', 'urls', 'pruning5')\n",
      "train time: 14.425s\n",
      "Bayesiano_TF-IDF('lemmatization', 'norm_letters', 'pruning10')\n",
      "train time: 14.543s\n",
      "Bayesiano_TF-IDF('lemmatization', 'norm_letters', 'pruning5')\n",
      "train time: 14.492s\n",
      "Bayesiano_TF-IDF('urls', 'norm_letters', 'pruning10')\n",
      "train time: 14.686s\n",
      "Bayesiano_TF-IDF('urls', 'norm_letters', 'pruning5')\n",
      "train time: 14.629s\n",
      "Bayesiano_TF-IDF('stop_words', 'stemming', 'urls', 'norm_letters')\n",
      "train time: 14.856s\n",
      "Bayesiano_TF-IDF('stop_words', 'stemming', 'urls', 'pruning10')\n",
      "train time: 14.590s\n",
      "Bayesiano_TF-IDF('stop_words', 'stemming', 'urls', 'pruning5')\n",
      "train time: 14.443s\n",
      "Bayesiano_TF-IDF('stop_words', 'stemming', 'norm_letters', 'pruning10')\n",
      "train time: 15.118s\n",
      "Bayesiano_TF-IDF('stop_words', 'stemming', 'norm_letters', 'pruning5')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 14.514s\n",
      "Bayesiano_TF-IDF('stop_words', 'lemmatization', 'urls', 'norm_letters')\n",
      "train time: 14.659s\n",
      "Bayesiano_TF-IDF('stop_words', 'lemmatization', 'urls', 'pruning10')\n",
      "train time: 14.787s\n",
      "Bayesiano_TF-IDF('stop_words', 'lemmatization', 'urls', 'pruning5')\n",
      "train time: 14.977s\n",
      "Bayesiano_TF-IDF('stop_words', 'lemmatization', 'norm_letters', 'pruning10')\n",
      "train time: 14.841s\n",
      "Bayesiano_TF-IDF('stop_words', 'lemmatization', 'norm_letters', 'pruning5')\n",
      "train time: 14.889s\n",
      "Bayesiano_TF-IDF('stop_words', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 14.902s\n",
      "Bayesiano_TF-IDF('stop_words', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 15.014s\n",
      "Bayesiano_TF-IDF('stemming', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 15.170s\n",
      "Bayesiano_TF-IDF('stemming', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 15.371s\n",
      "Bayesiano_TF-IDF('lemmatization', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 15.195s\n",
      "Bayesiano_TF-IDF('lemmatization', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 15.225s\n",
      "Bayesiano_TF-IDF('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 15.134s\n",
      "Bayesiano_TF-IDF('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 15.038s\n",
      "Bayesiano_TF-IDF('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 15.286s\n",
      "Bayesiano_TF-IDF('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 15.270s\n",
      "Bayesiano_Binario('stop_words',)\n",
      "train time: 15.523s\n",
      "Bayesiano_Binario('stemming',)\n",
      "train time: 15.461s\n",
      "Bayesiano_Binario('lemmatization',)\n",
      "train time: 15.679s\n",
      "Bayesiano_Binario('urls',)\n",
      "train time: 15.391s\n",
      "Bayesiano_Binario('norm_letters',)\n",
      "train time: 15.759s\n",
      "Bayesiano_Binario('pruning10',)\n",
      "train time: 15.786s\n",
      "Bayesiano_Binario('pruning5',)\n",
      "train time: 15.849s\n",
      "Bayesiano_Binario('stop_words', 'stemming')\n",
      "train time: 15.750s\n",
      "Bayesiano_Binario('stop_words', 'lemmatization')\n",
      "train time: 15.843s\n",
      "Bayesiano_Binario('stop_words', 'urls')\n",
      "train time: 15.859s\n",
      "Bayesiano_Binario('stop_words', 'norm_letters')\n",
      "train time: 16.084s\n",
      "Bayesiano_Binario('stop_words', 'pruning10')\n",
      "train time: 16.080s\n",
      "Bayesiano_Binario('stop_words', 'pruning5')\n",
      "train time: 16.076s\n",
      "Bayesiano_Binario('stemming', 'urls')\n",
      "train time: 16.050s\n",
      "Bayesiano_Binario('stemming', 'norm_letters')\n",
      "train time: 16.257s\n",
      "Bayesiano_Binario('stemming', 'pruning10')\n",
      "train time: 16.325s\n",
      "Bayesiano_Binario('stemming', 'pruning5')\n",
      "train time: 16.194s\n",
      "Bayesiano_Binario('lemmatization', 'urls')\n",
      "train time: 16.310s\n",
      "Bayesiano_Binario('lemmatization', 'norm_letters')\n",
      "train time: 16.396s\n",
      "Bayesiano_Binario('lemmatization', 'pruning10')\n",
      "train time: 16.317s\n",
      "Bayesiano_Binario('lemmatization', 'pruning5')\n",
      "train time: 16.283s\n",
      "Bayesiano_Binario('urls', 'norm_letters')\n",
      "train time: 16.352s\n",
      "Bayesiano_Binario('urls', 'pruning10')\n",
      "train time: 16.494s\n",
      "Bayesiano_Binario('urls', 'pruning5')\n",
      "train time: 16.700s\n",
      "Bayesiano_Binario('norm_letters', 'pruning10')\n",
      "train time: 16.570s\n",
      "Bayesiano_Binario('norm_letters', 'pruning5')\n",
      "train time: 16.622s\n",
      "Bayesiano_Binario('stop_words', 'stemming', 'urls')\n",
      "train time: 16.654s\n",
      "Bayesiano_Binario('stop_words', 'stemming', 'norm_letters')\n",
      "train time: 16.794s\n",
      "Bayesiano_Binario('stop_words', 'stemming', 'pruning10')\n",
      "train time: 16.917s\n",
      "Bayesiano_Binario('stop_words', 'stemming', 'pruning5')\n",
      "train time: 16.841s\n",
      "Bayesiano_Binario('stop_words', 'lemmatization', 'urls')\n",
      "train time: 17.052s\n",
      "Bayesiano_Binario('stop_words', 'lemmatization', 'norm_letters')\n",
      "train time: 17.185s\n",
      "Bayesiano_Binario('stop_words', 'lemmatization', 'pruning10')\n",
      "train time: 17.120s\n",
      "Bayesiano_Binario('stop_words', 'lemmatization', 'pruning5')\n",
      "train time: 17.057s\n",
      "Bayesiano_Binario('stop_words', 'urls', 'norm_letters')\n",
      "train time: 17.030s\n",
      "Bayesiano_Binario('stop_words', 'urls', 'pruning10')\n",
      "train time: 17.154s\n",
      "Bayesiano_Binario('stop_words', 'urls', 'pruning5')\n",
      "train time: 17.781s\n",
      "Bayesiano_Binario('stop_words', 'norm_letters', 'pruning10')\n",
      "train time: 17.757s\n",
      "Bayesiano_Binario('stop_words', 'norm_letters', 'pruning5')\n",
      "train time: 17.308s\n",
      "Bayesiano_Binario('stemming', 'urls', 'norm_letters')\n",
      "train time: 17.447s\n",
      "Bayesiano_Binario('stemming', 'urls', 'pruning10')\n",
      "train time: 17.500s\n",
      "Bayesiano_Binario('stemming', 'urls', 'pruning5')\n",
      "train time: 17.362s\n",
      "Bayesiano_Binario('stemming', 'norm_letters', 'pruning10')\n",
      "train time: 17.510s\n",
      "Bayesiano_Binario('stemming', 'norm_letters', 'pruning5')\n",
      "train time: 17.562s\n",
      "Bayesiano_Binario('lemmatization', 'urls', 'norm_letters')\n",
      "train time: 17.714s\n",
      "Bayesiano_Binario('lemmatization', 'urls', 'pruning10')\n",
      "train time: 17.594s\n",
      "Bayesiano_Binario('lemmatization', 'urls', 'pruning5')\n",
      "train time: 17.725s\n",
      "Bayesiano_Binario('lemmatization', 'norm_letters', 'pruning10')\n",
      "train time: 17.685s\n",
      "Bayesiano_Binario('lemmatization', 'norm_letters', 'pruning5')\n",
      "train time: 17.907s\n",
      "Bayesiano_Binario('urls', 'norm_letters', 'pruning10')\n",
      "train time: 17.738s\n",
      "Bayesiano_Binario('urls', 'norm_letters', 'pruning5')\n",
      "train time: 17.874s\n",
      "Bayesiano_Binario('stop_words', 'stemming', 'urls', 'norm_letters')\n",
      "train time: 17.904s\n",
      "Bayesiano_Binario('stop_words', 'stemming', 'urls', 'pruning10')\n",
      "train time: 17.890s\n",
      "Bayesiano_Binario('stop_words', 'stemming', 'urls', 'pruning5')\n",
      "train time: 17.768s\n",
      "Bayesiano_Binario('stop_words', 'stemming', 'norm_letters', 'pruning10')\n",
      "train time: 17.885s\n",
      "Bayesiano_Binario('stop_words', 'stemming', 'norm_letters', 'pruning5')\n",
      "train time: 17.930s\n",
      "Bayesiano_Binario('stop_words', 'lemmatization', 'urls', 'norm_letters')\n",
      "train time: 18.129s\n",
      "Bayesiano_Binario('stop_words', 'lemmatization', 'urls', 'pruning10')\n",
      "train time: 17.864s\n",
      "Bayesiano_Binario('stop_words', 'lemmatization', 'urls', 'pruning5')\n",
      "train time: 18.000s\n",
      "Bayesiano_Binario('stop_words', 'lemmatization', 'norm_letters', 'pruning10')\n",
      "train time: 18.182s\n",
      "Bayesiano_Binario('stop_words', 'lemmatization', 'norm_letters', 'pruning5')\n",
      "train time: 18.009s\n",
      "Bayesiano_Binario('stop_words', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 18.197s\n",
      "Bayesiano_Binario('stop_words', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 18.128s\n",
      "Bayesiano_Binario('stemming', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 18.432s\n",
      "Bayesiano_Binario('stemming', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 18.201s\n",
      "Bayesiano_Binario('lemmatization', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 18.103s\n",
      "Bayesiano_Binario('lemmatization', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 18.455s\n",
      "Bayesiano_Binario('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 18.295s\n",
      "Bayesiano_Binario('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 18.219s\n",
      "Bayesiano_Binario('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 18.357s\n",
      "Bayesiano_Binario('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 18.395s\n",
      "SVM_radial_TF('stop_words',)\n",
      "train time: 30.067s\n",
      "SVM_radial_TF('stemming',)\n",
      "train time: 30.737s\n",
      "SVM_radial_TF('lemmatization',)\n",
      "train time: 31.663s\n",
      "SVM_radial_TF('urls',)\n",
      "train time: 32.175s\n",
      "SVM_radial_TF('norm_letters',)\n",
      "train time: 32.121s\n",
      "SVM_radial_TF('pruning10',)\n",
      "train time: 26.383s\n",
      "SVM_radial_TF('pruning5',)\n",
      "train time: 27.455s\n",
      "SVM_radial_TF('stop_words', 'stemming')\n",
      "train time: 29.377s\n",
      "SVM_radial_TF('stop_words', 'lemmatization')\n",
      "train time: 28.952s\n",
      "SVM_radial_TF('stop_words', 'urls')\n",
      "train time: 29.178s\n",
      "SVM_radial_TF('stop_words', 'norm_letters')\n",
      "train time: 31.153s\n",
      "SVM_radial_TF('stop_words', 'pruning10')\n",
      "train time: 24.470s\n",
      "SVM_radial_TF('stop_words', 'pruning5')\n",
      "train time: 25.478s\n",
      "SVM_radial_TF('stemming', 'urls')\n",
      "train time: 32.380s\n",
      "SVM_radial_TF('stemming', 'norm_letters')\n",
      "train time: 31.428s\n",
      "SVM_radial_TF('stemming', 'pruning10')\n",
      "train time: 28.234s\n",
      "SVM_radial_TF('stemming', 'pruning5')\n",
      "train time: 28.496s\n",
      "SVM_radial_TF('lemmatization', 'urls')\n",
      "train time: 32.115s\n",
      "SVM_radial_TF('lemmatization', 'norm_letters')\n",
      "train time: 32.366s\n",
      "SVM_radial_TF('lemmatization', 'pruning10')\n",
      "train time: 28.066s\n",
      "SVM_radial_TF('lemmatization', 'pruning5')\n",
      "train time: 28.115s\n",
      "SVM_radial_TF('urls', 'norm_letters')\n",
      "train time: 32.651s\n",
      "SVM_radial_TF('urls', 'pruning10')\n",
      "train time: 28.376s\n",
      "SVM_radial_TF('urls', 'pruning5')\n",
      "train time: 29.423s\n",
      "SVM_radial_TF('norm_letters', 'pruning10')\n",
      "train time: 27.269s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM_radial_TF('norm_letters', 'pruning5')\n",
      "train time: 28.220s\n",
      "SVM_radial_TF('stop_words', 'stemming', 'urls')\n",
      "train time: 29.726s\n",
      "SVM_radial_TF('stop_words', 'stemming', 'norm_letters')\n",
      "train time: 29.592s\n",
      "SVM_radial_TF('stop_words', 'stemming', 'pruning10')\n",
      "train time: 26.328s\n",
      "SVM_radial_TF('stop_words', 'stemming', 'pruning5')\n",
      "train time: 27.206s\n",
      "SVM_radial_TF('stop_words', 'lemmatization', 'urls')\n",
      "train time: 29.986s\n",
      "SVM_radial_TF('stop_words', 'lemmatization', 'norm_letters')\n",
      "train time: 30.104s\n",
      "SVM_radial_TF('stop_words', 'lemmatization', 'pruning10')\n",
      "train time: 26.301s\n",
      "SVM_radial_TF('stop_words', 'lemmatization', 'pruning5')\n",
      "train time: 26.837s\n",
      "SVM_radial_TF('stop_words', 'urls', 'norm_letters')\n",
      "train time: 29.977s\n",
      "SVM_radial_TF('stop_words', 'urls', 'pruning10')\n",
      "train time: 26.240s\n",
      "SVM_radial_TF('stop_words', 'urls', 'pruning5')\n",
      "train time: 27.191s\n",
      "SVM_radial_TF('stop_words', 'norm_letters', 'pruning10')\n",
      "train time: 26.081s\n",
      "SVM_radial_TF('stop_words', 'norm_letters', 'pruning5')\n",
      "train time: 26.691s\n",
      "SVM_radial_TF('stemming', 'urls', 'norm_letters')\n",
      "train time: 32.798s\n",
      "SVM_radial_TF('stemming', 'urls', 'pruning10')\n",
      "train time: 30.530s\n",
      "SVM_radial_TF('stemming', 'urls', 'pruning5')\n",
      "train time: 31.051s\n",
      "SVM_radial_TF('stemming', 'norm_letters', 'pruning10')\n",
      "train time: 28.692s\n",
      "SVM_radial_TF('stemming', 'norm_letters', 'pruning5')\n",
      "train time: 29.955s\n",
      "SVM_radial_TF('lemmatization', 'urls', 'norm_letters')\n",
      "train time: 33.279s\n",
      "SVM_radial_TF('lemmatization', 'urls', 'pruning10')\n",
      "train time: 30.592s\n",
      "SVM_radial_TF('lemmatization', 'urls', 'pruning5')\n",
      "train time: 31.178s\n",
      "SVM_radial_TF('lemmatization', 'norm_letters', 'pruning10')\n",
      "train time: 28.846s\n",
      "SVM_radial_TF('lemmatization', 'norm_letters', 'pruning5')\n",
      "train time: 29.673s\n",
      "SVM_radial_TF('urls', 'norm_letters', 'pruning10')\n",
      "train time: 29.608s\n",
      "SVM_radial_TF('urls', 'norm_letters', 'pruning5')\n",
      "train time: 30.610s\n",
      "SVM_radial_TF('stop_words', 'stemming', 'urls', 'norm_letters')\n",
      "train time: 30.908s\n",
      "SVM_radial_TF('stop_words', 'stemming', 'urls', 'pruning10')\n",
      "train time: 28.140s\n",
      "SVM_radial_TF('stop_words', 'stemming', 'urls', 'pruning5')\n",
      "train time: 29.190s\n",
      "SVM_radial_TF('stop_words', 'stemming', 'norm_letters', 'pruning10')\n",
      "train time: 27.281s\n",
      "SVM_radial_TF('stop_words', 'stemming', 'norm_letters', 'pruning5')\n",
      "train time: 27.848s\n",
      "SVM_radial_TF('stop_words', 'lemmatization', 'urls', 'norm_letters')\n",
      "train time: 31.494s\n",
      "SVM_radial_TF('stop_words', 'lemmatization', 'urls', 'pruning10')\n",
      "train time: 28.243s\n",
      "SVM_radial_TF('stop_words', 'lemmatization', 'urls', 'pruning5')\n",
      "train time: 28.941s\n",
      "SVM_radial_TF('stop_words', 'lemmatization', 'norm_letters', 'pruning10')\n",
      "train time: 27.916s\n",
      "SVM_radial_TF('stop_words', 'lemmatization', 'norm_letters', 'pruning5')\n",
      "train time: 28.299s\n",
      "SVM_radial_TF('stop_words', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 27.209s\n",
      "SVM_radial_TF('stop_words', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 28.274s\n",
      "SVM_radial_TF('stemming', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 31.000s\n",
      "SVM_radial_TF('stemming', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 31.690s\n",
      "SVM_radial_TF('lemmatization', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 30.973s\n",
      "SVM_radial_TF('lemmatization', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 31.823s\n",
      "SVM_radial_TF('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 28.883s\n",
      "SVM_radial_TF('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 29.119s\n",
      "SVM_radial_TF('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 28.406s\n",
      "SVM_radial_TF('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 29.203s\n",
      "SVM_radial_TF-IDF('stop_words',)\n",
      "train time: 39.176s\n",
      "SVM_radial_TF-IDF('stemming',)\n",
      "train time: 39.340s\n",
      "SVM_radial_TF-IDF('lemmatization',)\n",
      "train time: 38.194s\n",
      "SVM_radial_TF-IDF('urls',)\n",
      "train time: 38.658s\n",
      "SVM_radial_TF-IDF('norm_letters',)\n",
      "train time: 40.245s\n",
      "SVM_radial_TF-IDF('pruning10',)\n",
      "train time: 30.401s\n",
      "SVM_radial_TF-IDF('pruning5',)\n",
      "train time: 30.376s\n",
      "SVM_radial_TF-IDF('stop_words', 'stemming')\n",
      "train time: 35.760s\n",
      "SVM_radial_TF-IDF('stop_words', 'lemmatization')\n",
      "train time: 36.139s\n",
      "SVM_radial_TF-IDF('stop_words', 'urls')\n",
      "train time: 34.952s\n",
      "SVM_radial_TF-IDF('stop_words', 'norm_letters')\n",
      "train time: 35.396s\n",
      "SVM_radial_TF-IDF('stop_words', 'pruning10')\n",
      "train time: 28.072s\n",
      "SVM_radial_TF-IDF('stop_words', 'pruning5')\n",
      "train time: 28.839s\n",
      "SVM_radial_TF-IDF('stemming', 'urls')\n",
      "train time: 35.698s\n",
      "SVM_radial_TF-IDF('stemming', 'norm_letters')\n",
      "train time: 39.645s\n",
      "SVM_radial_TF-IDF('stemming', 'pruning10')\n",
      "train time: 31.507s\n",
      "SVM_radial_TF-IDF('stemming', 'pruning5')\n",
      "train time: 31.811s\n",
      "SVM_radial_TF-IDF('lemmatization', 'urls')\n",
      "train time: 35.417s\n",
      "SVM_radial_TF-IDF('lemmatization', 'norm_letters')\n",
      "train time: 39.318s\n",
      "SVM_radial_TF-IDF('lemmatization', 'pruning10')\n",
      "train time: 30.587s\n",
      "SVM_radial_TF-IDF('lemmatization', 'pruning5')\n",
      "train time: 31.984s\n",
      "SVM_radial_TF-IDF('urls', 'norm_letters')\n",
      "train time: 40.062s\n",
      "SVM_radial_TF-IDF('urls', 'pruning10')\n",
      "train time: 31.296s\n",
      "SVM_radial_TF-IDF('urls', 'pruning5')\n",
      "train time: 32.056s\n",
      "SVM_radial_TF-IDF('norm_letters', 'pruning10')\n",
      "train time: 30.542s\n",
      "SVM_radial_TF-IDF('norm_letters', 'pruning5')\n",
      "train time: 31.836s\n",
      "SVM_radial_TF-IDF('stop_words', 'stemming', 'urls')\n",
      "train time: 34.007s\n",
      "SVM_radial_TF-IDF('stop_words', 'stemming', 'norm_letters')\n",
      "train time: 36.899s\n",
      "SVM_radial_TF-IDF('stop_words', 'stemming', 'pruning10')\n",
      "train time: 29.615s\n",
      "SVM_radial_TF-IDF('stop_words', 'stemming', 'pruning5')\n",
      "train time: 30.342s\n",
      "SVM_radial_TF-IDF('stop_words', 'lemmatization', 'urls')\n",
      "train time: 35.402s\n",
      "SVM_radial_TF-IDF('stop_words', 'lemmatization', 'norm_letters')\n",
      "train time: 36.655s\n",
      "SVM_radial_TF-IDF('stop_words', 'lemmatization', 'pruning10')\n",
      "train time: 29.392s\n",
      "SVM_radial_TF-IDF('stop_words', 'lemmatization', 'pruning5')\n",
      "train time: 30.191s\n",
      "SVM_radial_TF-IDF('stop_words', 'urls', 'norm_letters')\n",
      "train time: 37.146s\n",
      "SVM_radial_TF-IDF('stop_words', 'urls', 'pruning10')\n",
      "train time: 29.275s\n",
      "SVM_radial_TF-IDF('stop_words', 'urls', 'pruning5')\n",
      "train time: 30.373s\n",
      "SVM_radial_TF-IDF('stop_words', 'norm_letters', 'pruning10')\n",
      "train time: 29.289s\n",
      "SVM_radial_TF-IDF('stop_words', 'norm_letters', 'pruning5')\n",
      "train time: 30.122s\n",
      "SVM_radial_TF-IDF('stemming', 'urls', 'norm_letters')\n",
      "train time: 37.455s\n",
      "SVM_radial_TF-IDF('stemming', 'urls', 'pruning10')\n",
      "train time: 32.818s\n",
      "SVM_radial_TF-IDF('stemming', 'urls', 'pruning5')\n",
      "train time: 33.830s\n",
      "SVM_radial_TF-IDF('stemming', 'norm_letters', 'pruning10')\n",
      "train time: 32.138s\n",
      "SVM_radial_TF-IDF('stemming', 'norm_letters', 'pruning5')\n",
      "train time: 32.757s\n",
      "SVM_radial_TF-IDF('lemmatization', 'urls', 'norm_letters')\n",
      "train time: 37.185s\n",
      "SVM_radial_TF-IDF('lemmatization', 'urls', 'pruning10')\n",
      "train time: 32.978s\n",
      "SVM_radial_TF-IDF('lemmatization', 'urls', 'pruning5')\n",
      "train time: 33.580s\n",
      "SVM_radial_TF-IDF('lemmatization', 'norm_letters', 'pruning10')\n",
      "train time: 32.274s\n",
      "SVM_radial_TF-IDF('lemmatization', 'norm_letters', 'pruning5')\n",
      "train time: 32.616s\n",
      "train time: 37.880s\n",
      "SVM_radial_Binario('norm_letters',)\n",
      "train time: 38.802s\n",
      "SVM_radial_Binario('pruning10',)\n",
      "train time: 32.818s\n",
      "SVM_radial_Binario('pruning5',)\n",
      "train time: 33.525s\n",
      "SVM_radial_Binario('stop_words', 'stemming')\n",
      "train time: 35.397s\n",
      "SVM_radial_Binario('stop_words', 'lemmatization')\n",
      "train time: 35.308s\n",
      "SVM_radial_Binario('stop_words', 'urls')\n",
      "train time: 47.647s\n",
      "Reg_Log_TF-IDF('stop_words', 'lemmatization', 'norm_letters')\n",
      "train time: 46.854s\n",
      "Reg_Log_TF-IDF('stop_words', 'lemmatization', 'pruning10')\n",
      "train time: 44.357s\n",
      "Reg_Log_TF-IDF('stop_words', 'lemmatization', 'pruning5')\n",
      "train time: 43.229s\n",
      "Reg_Log_TF-IDF('stop_words', 'urls', 'norm_letters')\n",
      "train time: 44.375s\n",
      "Reg_Log_TF-IDF('stop_words', 'urls', 'pruning10')\n",
      "train time: 44.688s\n",
      "Reg_Log_TF-IDF('stemming', 'urls', 'norm_letters')\n",
      "train time: 45.626s\n",
      "Reg_Log_TF-IDF('stemming', 'urls', 'pruning10')\n",
      "train time: 50.929s\n",
      "Reg_Log_TF-IDF('stemming', 'urls', 'pruning5')\n",
      "train time: 48.771s\n",
      "Reg_Log_TF-IDF('stemming', 'norm_letters', 'pruning10')\n",
      "train time: 46.233s\n",
      "Reg_Log_TF-IDF('stemming', 'norm_letters', 'pruning5')\n",
      "train time: 44.550s\n",
      "Reg_Log_TF-IDF('lemmatization', 'urls', 'norm_letters')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 42.629s\n",
      "Reg_Log_TF-IDF('lemmatization', 'urls', 'pruning10')\n",
      "train time: 49.051s\n",
      "Reg_Log_TF-IDF('lemmatization', 'urls', 'pruning5')\n",
      "train time: 49.745s\n",
      "Reg_Log_TF-IDF('lemmatization', 'norm_letters', 'pruning10')\n",
      "train time: 44.329s\n",
      "Reg_Log_TF-IDF('lemmatization', 'norm_letters', 'pruning5')\n",
      "train time: 43.340s\n",
      "Reg_Log_TF-IDF('urls', 'norm_letters', 'pruning10')\n",
      "train time: 51.892s\n",
      "Reg_Log_TF-IDF('urls', 'norm_letters', 'pruning5')\n",
      "train time: 49.792s\n",
      "Reg_Log_TF-IDF('stop_words', 'stemming', 'urls', 'norm_letters')\n",
      "train time: 43.701s\n",
      "Reg_Log_TF-IDF('stop_words', 'stemming', 'urls', 'pruning10')\n",
      "train time: 43.897s\n",
      "Reg_Log_TF-IDF('stop_words', 'stemming', 'urls', 'pruning5')\n",
      "train time: 46.197s\n",
      "Reg_Log_TF-IDF('stop_words', 'stemming', 'norm_letters', 'pruning10')\n",
      "train time: 49.782s\n",
      "Reg_Log_TF-IDF('stop_words', 'stemming', 'norm_letters', 'pruning5')\n",
      "train time: 45.054s\n",
      "Reg_Log_TF-IDF('stop_words', 'lemmatization', 'urls', 'norm_letters')\n",
      "train time: 44.992s\n",
      "Reg_Log_TF-IDF('stop_words', 'lemmatization', 'urls', 'pruning10')\n",
      "train time: 46.576s\n",
      "Reg_Log_TF-IDF('stop_words', 'lemmatization', 'urls', 'pruning5')\n",
      "train time: 46.593s\n",
      "Reg_Log_TF-IDF('stop_words', 'lemmatization', 'norm_letters', 'pruning10')\n",
      "train time: 50.703s\n",
      "Reg_Log_TF-IDF('stop_words', 'lemmatization', 'norm_letters', 'pruning5')\n",
      "train time: 44.514s\n",
      "Reg_Log_TF-IDF('stop_words', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 45.099s\n",
      "Reg_Log_TF-IDF('stop_words', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 46.595s\n",
      "Reg_Log_TF-IDF('stemming', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 52.020s\n",
      "Reg_Log_TF-IDF('stemming', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 44.929s\n",
      "Reg_Log_TF-IDF('lemmatization', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 43.672s\n",
      "Reg_Log_TF-IDF('lemmatization', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 45.829s\n",
      "Reg_Log_TF-IDF('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 48.133s\n",
      "Reg_Log_TF-IDF('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 47.727s\n",
      "Reg_Log_TF-IDF('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 45.243s\n",
      "Reg_Log_TF-IDF('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 44.197s\n",
      "Reg_Log_Binario('stop_words',)\n",
      "train time: 45.026s\n",
      "Reg_Log_Binario('stemming',)\n",
      "train time: 48.806s\n",
      "Reg_Log_Binario('lemmatization',)\n",
      "train time: 50.116s\n",
      "Reg_Log_Binario('urls',)\n",
      "train time: 46.830s\n",
      "Reg_Log_Binario('norm_letters',)\n",
      "train time: 44.648s\n",
      "Reg_Log_Binario('pruning10',)\n",
      "train time: 44.959s\n",
      "Reg_Log_Binario('pruning5',)\n",
      "train time: 50.131s\n",
      "Reg_Log_Binario('stop_words', 'stemming')\n",
      "train time: 44.896s\n",
      "Reg_Log_Binario('stop_words', 'lemmatization')\n",
      "train time: 45.331s\n",
      "Reg_Log_Binario('stop_words', 'urls')\n",
      "train time: 53.418s\n",
      "Reg_Log_Binario('stop_words', 'norm_letters')\n",
      "train time: 47.276s\n",
      "Reg_Log_Binario('stop_words', 'pruning10')\n",
      "train time: 45.738s\n",
      "Reg_Log_Binario('stop_words', 'pruning5')\n",
      "train time: 46.730s\n",
      "Reg_Log_Binario('stemming', 'urls')\n",
      "train time: 50.805s\n",
      "Reg_Log_Binario('stemming', 'norm_letters')\n",
      "train time: 44.677s\n",
      "Reg_Log_Binario('stemming', 'pruning10')\n",
      "train time: 47.226s\n",
      "Reg_Log_Binario('stemming', 'pruning5')\n",
      "train time: 49.991s\n",
      "Reg_Log_Binario('lemmatization', 'urls')\n",
      "train time: 46.105s\n",
      "Reg_Log_Binario('lemmatization', 'norm_letters')\n",
      "train time: 45.787s\n",
      "Reg_Log_Binario('lemmatization', 'pruning10')\n",
      "train time: 50.221s\n",
      "Reg_Log_Binario('lemmatization', 'pruning5')\n",
      "train time: 51.256s\n",
      "Reg_Log_Binario('urls', 'norm_letters')\n",
      "train time: 47.767s\n",
      "Reg_Log_Binario('urls', 'pruning10')\n",
      "train time: 47.131s\n",
      "Reg_Log_Binario('urls', 'pruning5')\n",
      "train time: 48.421s\n",
      "Reg_Log_Binario('norm_letters', 'pruning10')\n",
      "train time: 50.723s\n",
      "Reg_Log_Binario('norm_letters', 'pruning5')\n",
      "train time: 45.713s\n",
      "Reg_Log_Binario('stop_words', 'stemming', 'urls')\n",
      "train time: 52.065s\n",
      "Reg_Log_Binario('stop_words', 'stemming', 'norm_letters')\n",
      "train time: 49.960s\n",
      "Reg_Log_Binario('stop_words', 'stemming', 'pruning10')\n",
      "train time: 45.314s\n",
      "Reg_Log_Binario('stop_words', 'stemming', 'pruning5')\n",
      "train time: 48.372s\n",
      "Reg_Log_Binario('stop_words', 'lemmatization', 'urls')\n",
      "train time: 51.307s\n",
      "Reg_Log_Binario('stop_words', 'lemmatization', 'norm_letters')\n",
      "train time: 44.593s\n",
      "Reg_Log_Binario('stop_words', 'lemmatization', 'pruning10')\n",
      "train time: 52.432s\n",
      "Reg_Log_Binario('stop_words', 'lemmatization', 'pruning5')\n",
      "train time: 50.883s\n",
      "Reg_Log_Binario('stop_words', 'urls', 'norm_letters')\n",
      "train time: 45.983s\n",
      "Reg_Log_Binario('stop_words', 'urls', 'pruning10')\n",
      "train time: 46.018s\n",
      "Reg_Log_Binario('stop_words', 'urls', 'pruning5')\n",
      "train time: 53.853s\n",
      "Reg_Log_Binario('stop_words', 'norm_letters', 'pruning10')\n",
      "train time: 47.201s\n",
      "Reg_Log_Binario('stop_words', 'norm_letters', 'pruning5')\n",
      "train time: 48.696s\n",
      "Reg_Log_Binario('stemming', 'urls', 'norm_letters')\n",
      "train time: 54.028s\n",
      "Reg_Log_Binario('stemming', 'urls', 'pruning10')\n",
      "train time: 46.317s\n",
      "Reg_Log_Binario('stemming', 'urls', 'pruning5')\n",
      "train time: 46.672s\n",
      "Reg_Log_Binario('stemming', 'norm_letters', 'pruning10')\n",
      "train time: 51.783s\n",
      "Reg_Log_Binario('stemming', 'norm_letters', 'pruning5')\n",
      "train time: 49.963s\n",
      "Reg_Log_Binario('lemmatization', 'urls', 'norm_letters')\n",
      "train time: 47.775s\n",
      "Reg_Log_Binario('lemmatization', 'urls', 'pruning10')\n",
      "train time: 46.306s\n",
      "Reg_Log_Binario('lemmatization', 'urls', 'pruning5')\n",
      "train time: 55.033s\n",
      "Reg_Log_Binario('lemmatization', 'norm_letters', 'pruning10')\n",
      "train time: 50.558s\n",
      "Reg_Log_Binario('lemmatization', 'norm_letters', 'pruning5')\n",
      "train time: 48.628s\n",
      "Reg_Log_Binario('urls', 'norm_letters', 'pruning10')\n",
      "train time: 46.594s\n",
      "Reg_Log_Binario('urls', 'norm_letters', 'pruning5')\n",
      "train time: 47.953s\n",
      "Reg_Log_Binario('stop_words', 'stemming', 'urls', 'norm_letters')\n",
      "train time: 55.194s\n",
      "Reg_Log_Binario('stop_words', 'stemming', 'urls', 'pruning10')\n",
      "train time: 49.037s\n",
      "Reg_Log_Binario('stop_words', 'stemming', 'urls', 'pruning5')\n",
      "train time: 47.279s\n",
      "Reg_Log_Binario('stop_words', 'stemming', 'norm_letters', 'pruning10')\n",
      "train time: 51.166s\n",
      "Reg_Log_Binario('stop_words', 'stemming', 'norm_letters', 'pruning5')\n",
      "train time: 53.731s\n",
      "Reg_Log_Binario('stop_words', 'lemmatization', 'urls', 'norm_letters')\n",
      "train time: 47.958s\n",
      "Reg_Log_Binario('stop_words', 'lemmatization', 'urls', 'pruning10')\n",
      "train time: 45.920s\n",
      "Reg_Log_Binario('stop_words', 'lemmatization', 'urls', 'pruning5')\n",
      "train time: 47.675s\n",
      "Reg_Log_Binario('stop_words', 'lemmatization', 'norm_letters', 'pruning10')\n",
      "train time: 51.749s\n",
      "Reg_Log_Binario('stop_words', 'lemmatization', 'norm_letters', 'pruning5')\n",
      "train time: 47.312s\n",
      "Reg_Log_Binario('stop_words', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 48.708s\n",
      "Reg_Log_Binario('stop_words', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 54.931s\n",
      "Reg_Log_Binario('stemming', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 48.812s\n",
      "Reg_Log_Binario('stemming', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 47.069s\n",
      "Reg_Log_Binario('lemmatization', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 54.998s\n",
      "Reg_Log_Binario('lemmatization', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 49.481s\n",
      "Reg_Log_Binario('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 50.813s\n",
      "Reg_Log_Binario('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 55.629s\n",
      "Reg_Log_Binario('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning10')\n",
      "train time: 49.335s\n",
      "Reg_Log_Binario('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning5')\n",
      "train time: 48.614s\n"
     ]
    }
   ],
   "source": [
    "vectorizers_list = []\n",
    "results_list = []\n",
    "workbook = xlsxwriter.Workbook('result_combinations_news_fakes.xlsx')\n",
    "for clf_type_weighting_type in clf_types_weighting_types:\n",
    "    ws_name = clf_type_weighting_type['clf_name'] + '_' + clf_type_weighting_type['weighting_type']\n",
    "    worksheet = workbook.add_worksheet(ws_name)\n",
    "    init_row = 0\n",
    "    for comb in all_the_tasks:\n",
    "        \n",
    "        if clf_type_weighting_type['weighting_type'] == 'TF':\n",
    "            vectorizer = CountVectorizer()\n",
    "        if clf_type_weighting_type['weighting_type'] == 'TF-IDF':\n",
    "            vectorizer = TfidfVectorizer()\n",
    "        if clf_type_weighting_type['weighting_type'] == 'Binario':\n",
    "            vectorizer = CountVectorizer(binary=True)\n",
    "        \n",
    "        clf_type = clf_type_weighting_type['clf_type']\n",
    "        optimal_parameters = clf_type_weighting_type['optimal_parameters']\n",
    "        random_state = clf_type_weighting_type['random_state']\n",
    "        \n",
    "        def tokenize_combinations(text):\n",
    "            if ('urls' in comb):\n",
    "                url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "                text = re.sub(url_regex, '', text, flags=re.MULTILINE)\n",
    "\n",
    "            regexp_tokenizer = RegexpTokenizer(u'(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "            tokens =  regexp_tokenizer.tokenize(text)\n",
    "\n",
    "            if ('norm_letters' in comb):\n",
    "                tokens = norm_repeated_letters(tokens, replacer)\n",
    "\n",
    "            if ('stemming' in comb):\n",
    "                tokens = stem_tokens(tokens, stemmer)\n",
    "\n",
    "            if ('lemmatization' in comb):\n",
    "                tokens = lemmatizer(tokens)\n",
    "\n",
    "            return tokens\n",
    "\n",
    "        vectorizer.set_params(tokenizer = tokenize_combinations)\n",
    "        \n",
    "        if ('stop_words' in comb):\n",
    "            vectorizer.set_params(stop_words = spanish_stopwords)\n",
    "            \n",
    "        if ('pruning10' in comb):\n",
    "            vectorizer.set_params(min_df=10)\n",
    "            \n",
    "        if ('pruning5' in comb):\n",
    "            vectorizer.set_params(min_df=5)\n",
    "            \n",
    "        print(ws_name + str(comb))\n",
    "        exp_results = run_one_comb_experiment(total_data_content, total_data_target, vectorizer, optimal_parameters, clf_type, random_state)\n",
    "        continue_row = save_excel_comb_results(worksheet, init_row, exp_results, str(optimal_parameters), random_state, str(comb))\n",
    "        init_row = continue_row\n",
    "        vectorizers_list.append(vectorizer)\n",
    "        results_list.append(exp_results + (ws_name, str(comb)))\n",
    "workbook.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8926666666666667,\n",
       " 23179,\n",
       " 0.8924709159429788,\n",
       " 0.8945289958882208,\n",
       " 0.8923292329232924,\n",
       " 1403,\n",
       " 1275,\n",
       " 210,\n",
       " 112,\n",
       " 'Bayesiano_TF',\n",
       " \"('stop_words',)\")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "852\n"
     ]
    }
   ],
   "source": [
    "print(len(results_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_backup = results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.892667</td>\n",
       "      <td>23179</td>\n",
       "      <td>0.892471</td>\n",
       "      <td>0.894529</td>\n",
       "      <td>0.892329</td>\n",
       "      <td>1403</td>\n",
       "      <td>1275</td>\n",
       "      <td>210</td>\n",
       "      <td>112</td>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.879000</td>\n",
       "      <td>17645</td>\n",
       "      <td>0.878840</td>\n",
       "      <td>0.880180</td>\n",
       "      <td>0.878725</td>\n",
       "      <td>1373</td>\n",
       "      <td>1264</td>\n",
       "      <td>221</td>\n",
       "      <td>142</td>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stemming',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.877333</td>\n",
       "      <td>18669</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.877826</td>\n",
       "      <td>0.877154</td>\n",
       "      <td>1356</td>\n",
       "      <td>1276</td>\n",
       "      <td>209</td>\n",
       "      <td>159</td>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('lemmatization',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.856333</td>\n",
       "      <td>15636</td>\n",
       "      <td>0.856189</td>\n",
       "      <td>0.857082</td>\n",
       "      <td>0.856102</td>\n",
       "      <td>1332</td>\n",
       "      <td>1237</td>\n",
       "      <td>248</td>\n",
       "      <td>183</td>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('urls',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.891000</td>\n",
       "      <td>23296</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.892348</td>\n",
       "      <td>0.890712</td>\n",
       "      <td>1393</td>\n",
       "      <td>1280</td>\n",
       "      <td>205</td>\n",
       "      <td>122</td>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('norm_letters',)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1         2         3         4     5     6    7    8   \\\n",
       "0  0.892667  23179  0.892471  0.894529  0.892329  1403  1275  210  112   \n",
       "1  0.879000  17645  0.878840  0.880180  0.878725  1373  1264  221  142   \n",
       "2  0.877333  18669  0.877246  0.877826  0.877154  1356  1276  209  159   \n",
       "3  0.856333  15636  0.856189  0.857082  0.856102  1332  1237  248  183   \n",
       "4  0.891000  23296  0.890845  0.892348  0.890712  1393  1280  205  122   \n",
       "\n",
       "             9                   10  \n",
       "0  Bayesiano_TF     ('stop_words',)  \n",
       "1  Bayesiano_TF       ('stemming',)  \n",
       "2  Bayesiano_TF  ('lemmatization',)  \n",
       "3  Bayesiano_TF           ('urls',)  \n",
       "4  Bayesiano_TF   ('norm_letters',)  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df[[9, 10, 1, 0, 2, 3, 4, 5, 6, 7, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words',)</td>\n",
       "      <td>23179</td>\n",
       "      <td>0.892667</td>\n",
       "      <td>0.892471</td>\n",
       "      <td>0.894529</td>\n",
       "      <td>0.892329</td>\n",
       "      <td>1403</td>\n",
       "      <td>1275</td>\n",
       "      <td>210</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stemming',)</td>\n",
       "      <td>17645</td>\n",
       "      <td>0.879000</td>\n",
       "      <td>0.878840</td>\n",
       "      <td>0.880180</td>\n",
       "      <td>0.878725</td>\n",
       "      <td>1373</td>\n",
       "      <td>1264</td>\n",
       "      <td>221</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('lemmatization',)</td>\n",
       "      <td>18669</td>\n",
       "      <td>0.877333</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.877826</td>\n",
       "      <td>0.877154</td>\n",
       "      <td>1356</td>\n",
       "      <td>1276</td>\n",
       "      <td>209</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('urls',)</td>\n",
       "      <td>15636</td>\n",
       "      <td>0.856333</td>\n",
       "      <td>0.856189</td>\n",
       "      <td>0.857082</td>\n",
       "      <td>0.856102</td>\n",
       "      <td>1332</td>\n",
       "      <td>1237</td>\n",
       "      <td>248</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('norm_letters',)</td>\n",
       "      <td>23296</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.892348</td>\n",
       "      <td>0.890712</td>\n",
       "      <td>1393</td>\n",
       "      <td>1280</td>\n",
       "      <td>205</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('pruning10',)</td>\n",
       "      <td>1226</td>\n",
       "      <td>0.847333</td>\n",
       "      <td>0.846951</td>\n",
       "      <td>0.849716</td>\n",
       "      <td>0.846918</td>\n",
       "      <td>1346</td>\n",
       "      <td>1196</td>\n",
       "      <td>289</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('pruning5',)</td>\n",
       "      <td>2611</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.863169</td>\n",
       "      <td>0.864319</td>\n",
       "      <td>0.863073</td>\n",
       "      <td>1347</td>\n",
       "      <td>1243</td>\n",
       "      <td>242</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'stemming')</td>\n",
       "      <td>17565</td>\n",
       "      <td>0.873667</td>\n",
       "      <td>0.873500</td>\n",
       "      <td>0.874825</td>\n",
       "      <td>0.873391</td>\n",
       "      <td>1365</td>\n",
       "      <td>1256</td>\n",
       "      <td>229</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'lemmatization')</td>\n",
       "      <td>18607</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>0.881869</td>\n",
       "      <td>0.882946</td>\n",
       "      <td>0.881755</td>\n",
       "      <td>1373</td>\n",
       "      <td>1273</td>\n",
       "      <td>212</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'urls')</td>\n",
       "      <td>15446</td>\n",
       "      <td>0.851667</td>\n",
       "      <td>0.851564</td>\n",
       "      <td>0.852094</td>\n",
       "      <td>0.851488</td>\n",
       "      <td>1317</td>\n",
       "      <td>1238</td>\n",
       "      <td>247</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'norm_letters')</td>\n",
       "      <td>23107</td>\n",
       "      <td>0.891333</td>\n",
       "      <td>0.891141</td>\n",
       "      <td>0.893116</td>\n",
       "      <td>0.891002</td>\n",
       "      <td>1400</td>\n",
       "      <td>1274</td>\n",
       "      <td>211</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'pruning10')</td>\n",
       "      <td>1106</td>\n",
       "      <td>0.839333</td>\n",
       "      <td>0.838898</td>\n",
       "      <td>0.841884</td>\n",
       "      <td>0.838897</td>\n",
       "      <td>1337</td>\n",
       "      <td>1181</td>\n",
       "      <td>304</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'pruning5')</td>\n",
       "      <td>2473</td>\n",
       "      <td>0.861000</td>\n",
       "      <td>0.860823</td>\n",
       "      <td>0.862055</td>\n",
       "      <td>0.860729</td>\n",
       "      <td>1345</td>\n",
       "      <td>1238</td>\n",
       "      <td>247</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stemming', 'urls')</td>\n",
       "      <td>9912</td>\n",
       "      <td>0.845333</td>\n",
       "      <td>0.845223</td>\n",
       "      <td>0.845766</td>\n",
       "      <td>0.845151</td>\n",
       "      <td>1308</td>\n",
       "      <td>1228</td>\n",
       "      <td>257</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stemming', 'norm_letters')</td>\n",
       "      <td>17560</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>0.877853</td>\n",
       "      <td>0.879038</td>\n",
       "      <td>0.877741</td>\n",
       "      <td>1369</td>\n",
       "      <td>1265</td>\n",
       "      <td>220</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stemming', 'pruning10')</td>\n",
       "      <td>1350</td>\n",
       "      <td>0.840667</td>\n",
       "      <td>0.840288</td>\n",
       "      <td>0.842845</td>\n",
       "      <td>0.840264</td>\n",
       "      <td>1334</td>\n",
       "      <td>1188</td>\n",
       "      <td>297</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stemming', 'pruning5')</td>\n",
       "      <td>2524</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.858840</td>\n",
       "      <td>0.859896</td>\n",
       "      <td>0.858749</td>\n",
       "      <td>1339</td>\n",
       "      <td>1238</td>\n",
       "      <td>247</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('lemmatization', 'urls')</td>\n",
       "      <td>10936</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.841932</td>\n",
       "      <td>0.842179</td>\n",
       "      <td>0.841878</td>\n",
       "      <td>1294</td>\n",
       "      <td>1232</td>\n",
       "      <td>253</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('lemmatization', 'norm_letters')</td>\n",
       "      <td>18621</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>0.877904</td>\n",
       "      <td>0.878571</td>\n",
       "      <td>0.877808</td>\n",
       "      <td>1359</td>\n",
       "      <td>1275</td>\n",
       "      <td>210</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('lemmatization', 'pruning10')</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.835333</td>\n",
       "      <td>0.835014</td>\n",
       "      <td>0.836994</td>\n",
       "      <td>0.834977</td>\n",
       "      <td>1319</td>\n",
       "      <td>1187</td>\n",
       "      <td>298</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('lemmatization', 'pruning5')</td>\n",
       "      <td>2427</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.853885</td>\n",
       "      <td>0.854520</td>\n",
       "      <td>0.853805</td>\n",
       "      <td>1323</td>\n",
       "      <td>1239</td>\n",
       "      <td>246</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('urls', 'norm_letters')</td>\n",
       "      <td>15563</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.855846</td>\n",
       "      <td>0.856816</td>\n",
       "      <td>0.855759</td>\n",
       "      <td>1333</td>\n",
       "      <td>1235</td>\n",
       "      <td>250</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('urls', 'pruning10')</td>\n",
       "      <td>1220</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.799812</td>\n",
       "      <td>0.800535</td>\n",
       "      <td>0.799773</td>\n",
       "      <td>1246</td>\n",
       "      <td>1154</td>\n",
       "      <td>331</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('urls', 'pruning5')</td>\n",
       "      <td>2589</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.821530</td>\n",
       "      <td>0.822102</td>\n",
       "      <td>0.821472</td>\n",
       "      <td>1274</td>\n",
       "      <td>1191</td>\n",
       "      <td>294</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('norm_letters', 'pruning10')</td>\n",
       "      <td>1226</td>\n",
       "      <td>0.846333</td>\n",
       "      <td>0.845953</td>\n",
       "      <td>0.848669</td>\n",
       "      <td>0.845921</td>\n",
       "      <td>1344</td>\n",
       "      <td>1195</td>\n",
       "      <td>290</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('norm_letters', 'pruning5')</td>\n",
       "      <td>2607</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.863836</td>\n",
       "      <td>0.864988</td>\n",
       "      <td>0.863740</td>\n",
       "      <td>1348</td>\n",
       "      <td>1244</td>\n",
       "      <td>241</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'stemming', 'urls')</td>\n",
       "      <td>9832</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.846597</td>\n",
       "      <td>0.846873</td>\n",
       "      <td>0.846538</td>\n",
       "      <td>1302</td>\n",
       "      <td>1238</td>\n",
       "      <td>247</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'stemming', 'norm_letters')</td>\n",
       "      <td>17481</td>\n",
       "      <td>0.874333</td>\n",
       "      <td>0.874179</td>\n",
       "      <td>0.875385</td>\n",
       "      <td>0.874071</td>\n",
       "      <td>1364</td>\n",
       "      <td>1259</td>\n",
       "      <td>226</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'stemming', 'pruning10')</td>\n",
       "      <td>1283</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>0.830647</td>\n",
       "      <td>0.832795</td>\n",
       "      <td>0.830626</td>\n",
       "      <td>1315</td>\n",
       "      <td>1178</td>\n",
       "      <td>307</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'stemming', 'pruning5')</td>\n",
       "      <td>2451</td>\n",
       "      <td>0.851333</td>\n",
       "      <td>0.851161</td>\n",
       "      <td>0.852228</td>\n",
       "      <td>0.851078</td>\n",
       "      <td>1328</td>\n",
       "      <td>1226</td>\n",
       "      <td>259</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stemming', 'urls', 'pruning5')</td>\n",
       "      <td>2470</td>\n",
       "      <td>0.777000</td>\n",
       "      <td>0.776985</td>\n",
       "      <td>0.777129</td>\n",
       "      <td>0.777028</td>\n",
       "      <td>1153</td>\n",
       "      <td>1178</td>\n",
       "      <td>318</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stemming', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1364</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.816997</td>\n",
       "      <td>0.817056</td>\n",
       "      <td>0.817017</td>\n",
       "      <td>1219</td>\n",
       "      <td>1232</td>\n",
       "      <td>264</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stemming', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2495</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.822991</td>\n",
       "      <td>0.823112</td>\n",
       "      <td>0.823025</td>\n",
       "      <td>1224</td>\n",
       "      <td>1245</td>\n",
       "      <td>251</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'urls', 'norm_letters')</td>\n",
       "      <td>10821</td>\n",
       "      <td>0.822333</td>\n",
       "      <td>0.822234</td>\n",
       "      <td>0.823208</td>\n",
       "      <td>0.822402</td>\n",
       "      <td>1198</td>\n",
       "      <td>1269</td>\n",
       "      <td>227</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'urls', 'pruning10')</td>\n",
       "      <td>1260</td>\n",
       "      <td>0.771000</td>\n",
       "      <td>0.770886</td>\n",
       "      <td>0.771672</td>\n",
       "      <td>0.771065</td>\n",
       "      <td>1123</td>\n",
       "      <td>1190</td>\n",
       "      <td>306</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'urls', 'pruning5')</td>\n",
       "      <td>2383</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.752988</td>\n",
       "      <td>0.753093</td>\n",
       "      <td>0.753024</td>\n",
       "      <td>1119</td>\n",
       "      <td>1140</td>\n",
       "      <td>356</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1261</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.816995</td>\n",
       "      <td>0.817068</td>\n",
       "      <td>0.817019</td>\n",
       "      <td>1218</td>\n",
       "      <td>1233</td>\n",
       "      <td>263</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2396</td>\n",
       "      <td>0.826333</td>\n",
       "      <td>0.826319</td>\n",
       "      <td>0.826500</td>\n",
       "      <td>0.826363</td>\n",
       "      <td>1226</td>\n",
       "      <td>1253</td>\n",
       "      <td>243</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('urls', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1207</td>\n",
       "      <td>0.782333</td>\n",
       "      <td>0.782293</td>\n",
       "      <td>0.782628</td>\n",
       "      <td>0.782375</td>\n",
       "      <td>1153</td>\n",
       "      <td>1194</td>\n",
       "      <td>302</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2595</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>0.763998</td>\n",
       "      <td>0.764028</td>\n",
       "      <td>0.764013</td>\n",
       "      <td>1142</td>\n",
       "      <td>1150</td>\n",
       "      <td>346</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'urls', 'norm_letters')</td>\n",
       "      <td>9688</td>\n",
       "      <td>0.818333</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>0.819590</td>\n",
       "      <td>0.818416</td>\n",
       "      <td>1184</td>\n",
       "      <td>1271</td>\n",
       "      <td>225</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'urls', 'pruning10')</td>\n",
       "      <td>1296</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>0.763918</td>\n",
       "      <td>0.764476</td>\n",
       "      <td>0.764055</td>\n",
       "      <td>1118</td>\n",
       "      <td>1174</td>\n",
       "      <td>322</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'urls', 'pruning5')</td>\n",
       "      <td>2396</td>\n",
       "      <td>0.770333</td>\n",
       "      <td>0.770272</td>\n",
       "      <td>0.770718</td>\n",
       "      <td>0.770382</td>\n",
       "      <td>1131</td>\n",
       "      <td>1180</td>\n",
       "      <td>316</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1298</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.810997</td>\n",
       "      <td>0.811044</td>\n",
       "      <td>0.811016</td>\n",
       "      <td>1211</td>\n",
       "      <td>1222</td>\n",
       "      <td>274</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2422</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.818991</td>\n",
       "      <td>0.819111</td>\n",
       "      <td>0.819024</td>\n",
       "      <td>1218</td>\n",
       "      <td>1239</td>\n",
       "      <td>257</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'urls', 'norm_letters')</td>\n",
       "      <td>10757</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.823796</td>\n",
       "      <td>0.825721</td>\n",
       "      <td>0.824097</td>\n",
       "      <td>1185</td>\n",
       "      <td>1287</td>\n",
       "      <td>209</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'urls', 'pruning10')</td>\n",
       "      <td>1206</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.766813</td>\n",
       "      <td>0.768023</td>\n",
       "      <td>0.767081</td>\n",
       "      <td>1108</td>\n",
       "      <td>1193</td>\n",
       "      <td>303</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'urls', 'pruning5')</td>\n",
       "      <td>2327</td>\n",
       "      <td>0.748333</td>\n",
       "      <td>0.748319</td>\n",
       "      <td>0.748356</td>\n",
       "      <td>0.748318</td>\n",
       "      <td>1134</td>\n",
       "      <td>1111</td>\n",
       "      <td>385</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1206</td>\n",
       "      <td>0.817667</td>\n",
       "      <td>0.817662</td>\n",
       "      <td>0.817735</td>\n",
       "      <td>0.817686</td>\n",
       "      <td>1219</td>\n",
       "      <td>1234</td>\n",
       "      <td>262</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2340</td>\n",
       "      <td>0.809333</td>\n",
       "      <td>0.809309</td>\n",
       "      <td>0.809566</td>\n",
       "      <td>0.809369</td>\n",
       "      <td>1197</td>\n",
       "      <td>1231</td>\n",
       "      <td>265</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'urls', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1089</td>\n",
       "      <td>0.769333</td>\n",
       "      <td>0.768976</td>\n",
       "      <td>0.771236</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>1095</td>\n",
       "      <td>1213</td>\n",
       "      <td>283</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2454</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.755982</td>\n",
       "      <td>0.756129</td>\n",
       "      <td>0.756028</td>\n",
       "      <td>1121</td>\n",
       "      <td>1147</td>\n",
       "      <td>349</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stemming', 'urls', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1358</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.773977</td>\n",
       "      <td>0.774171</td>\n",
       "      <td>0.774032</td>\n",
       "      <td>1146</td>\n",
       "      <td>1176</td>\n",
       "      <td>320</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stemming', 'urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2471</td>\n",
       "      <td>0.775667</td>\n",
       "      <td>0.775625</td>\n",
       "      <td>0.775955</td>\n",
       "      <td>0.775709</td>\n",
       "      <td>1143</td>\n",
       "      <td>1184</td>\n",
       "      <td>312</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'urls', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1255</td>\n",
       "      <td>0.770667</td>\n",
       "      <td>0.770549</td>\n",
       "      <td>0.771356</td>\n",
       "      <td>0.770733</td>\n",
       "      <td>1122</td>\n",
       "      <td>1190</td>\n",
       "      <td>306</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2372</td>\n",
       "      <td>0.760333</td>\n",
       "      <td>0.760329</td>\n",
       "      <td>0.760382</td>\n",
       "      <td>0.760350</td>\n",
       "      <td>1134</td>\n",
       "      <td>1147</td>\n",
       "      <td>349</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1292</td>\n",
       "      <td>0.763667</td>\n",
       "      <td>0.763581</td>\n",
       "      <td>0.764158</td>\n",
       "      <td>0.763723</td>\n",
       "      <td>1117</td>\n",
       "      <td>1174</td>\n",
       "      <td>322</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2398</td>\n",
       "      <td>0.769667</td>\n",
       "      <td>0.769595</td>\n",
       "      <td>0.770107</td>\n",
       "      <td>0.769719</td>\n",
       "      <td>1128</td>\n",
       "      <td>1181</td>\n",
       "      <td>315</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.766785</td>\n",
       "      <td>0.768160</td>\n",
       "      <td>0.767086</td>\n",
       "      <td>1105</td>\n",
       "      <td>1196</td>\n",
       "      <td>300</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2316</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.754994</td>\n",
       "      <td>0.755004</td>\n",
       "      <td>0.754992</td>\n",
       "      <td>1140</td>\n",
       "      <td>1125</td>\n",
       "      <td>371</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>852 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  9   \\\n",
       "0       Bayesiano_TF   \n",
       "1       Bayesiano_TF   \n",
       "2       Bayesiano_TF   \n",
       "3       Bayesiano_TF   \n",
       "4       Bayesiano_TF   \n",
       "5       Bayesiano_TF   \n",
       "6       Bayesiano_TF   \n",
       "7       Bayesiano_TF   \n",
       "8       Bayesiano_TF   \n",
       "9       Bayesiano_TF   \n",
       "10      Bayesiano_TF   \n",
       "11      Bayesiano_TF   \n",
       "12      Bayesiano_TF   \n",
       "13      Bayesiano_TF   \n",
       "14      Bayesiano_TF   \n",
       "15      Bayesiano_TF   \n",
       "16      Bayesiano_TF   \n",
       "17      Bayesiano_TF   \n",
       "18      Bayesiano_TF   \n",
       "19      Bayesiano_TF   \n",
       "20      Bayesiano_TF   \n",
       "21      Bayesiano_TF   \n",
       "22      Bayesiano_TF   \n",
       "23      Bayesiano_TF   \n",
       "24      Bayesiano_TF   \n",
       "25      Bayesiano_TF   \n",
       "26      Bayesiano_TF   \n",
       "27      Bayesiano_TF   \n",
       "28      Bayesiano_TF   \n",
       "29      Bayesiano_TF   \n",
       "..               ...   \n",
       "822  Reg_Log_Binario   \n",
       "823  Reg_Log_Binario   \n",
       "824  Reg_Log_Binario   \n",
       "825  Reg_Log_Binario   \n",
       "826  Reg_Log_Binario   \n",
       "827  Reg_Log_Binario   \n",
       "828  Reg_Log_Binario   \n",
       "829  Reg_Log_Binario   \n",
       "830  Reg_Log_Binario   \n",
       "831  Reg_Log_Binario   \n",
       "832  Reg_Log_Binario   \n",
       "833  Reg_Log_Binario   \n",
       "834  Reg_Log_Binario   \n",
       "835  Reg_Log_Binario   \n",
       "836  Reg_Log_Binario   \n",
       "837  Reg_Log_Binario   \n",
       "838  Reg_Log_Binario   \n",
       "839  Reg_Log_Binario   \n",
       "840  Reg_Log_Binario   \n",
       "841  Reg_Log_Binario   \n",
       "842  Reg_Log_Binario   \n",
       "843  Reg_Log_Binario   \n",
       "844  Reg_Log_Binario   \n",
       "845  Reg_Log_Binario   \n",
       "846  Reg_Log_Binario   \n",
       "847  Reg_Log_Binario   \n",
       "848  Reg_Log_Binario   \n",
       "849  Reg_Log_Binario   \n",
       "850  Reg_Log_Binario   \n",
       "851  Reg_Log_Binario   \n",
       "\n",
       "                                                                       10  \\\n",
       "0                                                         ('stop_words',)   \n",
       "1                                                           ('stemming',)   \n",
       "2                                                      ('lemmatization',)   \n",
       "3                                                               ('urls',)   \n",
       "4                                                       ('norm_letters',)   \n",
       "5                                                          ('pruning10',)   \n",
       "6                                                           ('pruning5',)   \n",
       "7                                              ('stop_words', 'stemming')   \n",
       "8                                         ('stop_words', 'lemmatization')   \n",
       "9                                                  ('stop_words', 'urls')   \n",
       "10                                         ('stop_words', 'norm_letters')   \n",
       "11                                            ('stop_words', 'pruning10')   \n",
       "12                                             ('stop_words', 'pruning5')   \n",
       "13                                                   ('stemming', 'urls')   \n",
       "14                                           ('stemming', 'norm_letters')   \n",
       "15                                              ('stemming', 'pruning10')   \n",
       "16                                               ('stemming', 'pruning5')   \n",
       "17                                              ('lemmatization', 'urls')   \n",
       "18                                      ('lemmatization', 'norm_letters')   \n",
       "19                                         ('lemmatization', 'pruning10')   \n",
       "20                                          ('lemmatization', 'pruning5')   \n",
       "21                                               ('urls', 'norm_letters')   \n",
       "22                                                  ('urls', 'pruning10')   \n",
       "23                                                   ('urls', 'pruning5')   \n",
       "24                                          ('norm_letters', 'pruning10')   \n",
       "25                                           ('norm_letters', 'pruning5')   \n",
       "26                                     ('stop_words', 'stemming', 'urls')   \n",
       "27                             ('stop_words', 'stemming', 'norm_letters')   \n",
       "28                                ('stop_words', 'stemming', 'pruning10')   \n",
       "29                                 ('stop_words', 'stemming', 'pruning5')   \n",
       "..                                                                    ...   \n",
       "822                                      ('stemming', 'urls', 'pruning5')   \n",
       "823                             ('stemming', 'norm_letters', 'pruning10')   \n",
       "824                              ('stemming', 'norm_letters', 'pruning5')   \n",
       "825                             ('lemmatization', 'urls', 'norm_letters')   \n",
       "826                                ('lemmatization', 'urls', 'pruning10')   \n",
       "827                                 ('lemmatization', 'urls', 'pruning5')   \n",
       "828                        ('lemmatization', 'norm_letters', 'pruning10')   \n",
       "829                         ('lemmatization', 'norm_letters', 'pruning5')   \n",
       "830                                 ('urls', 'norm_letters', 'pruning10')   \n",
       "831                                  ('urls', 'norm_letters', 'pruning5')   \n",
       "832                    ('stop_words', 'stemming', 'urls', 'norm_letters')   \n",
       "833                       ('stop_words', 'stemming', 'urls', 'pruning10')   \n",
       "834                        ('stop_words', 'stemming', 'urls', 'pruning5')   \n",
       "835               ('stop_words', 'stemming', 'norm_letters', 'pruning10')   \n",
       "836                ('stop_words', 'stemming', 'norm_letters', 'pruning5')   \n",
       "837               ('stop_words', 'lemmatization', 'urls', 'norm_letters')   \n",
       "838                  ('stop_words', 'lemmatization', 'urls', 'pruning10')   \n",
       "839                   ('stop_words', 'lemmatization', 'urls', 'pruning5')   \n",
       "840          ('stop_words', 'lemmatization', 'norm_letters', 'pruning10')   \n",
       "841           ('stop_words', 'lemmatization', 'norm_letters', 'pruning5')   \n",
       "842                   ('stop_words', 'urls', 'norm_letters', 'pruning10')   \n",
       "843                    ('stop_words', 'urls', 'norm_letters', 'pruning5')   \n",
       "844                     ('stemming', 'urls', 'norm_letters', 'pruning10')   \n",
       "845                      ('stemming', 'urls', 'norm_letters', 'pruning5')   \n",
       "846                ('lemmatization', 'urls', 'norm_letters', 'pruning10')   \n",
       "847                 ('lemmatization', 'urls', 'norm_letters', 'pruning5')   \n",
       "848       ('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning10')   \n",
       "849        ('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning5')   \n",
       "850  ('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning10')   \n",
       "851   ('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning5')   \n",
       "\n",
       "        1         0         2         3         4     5     6    7    8   \n",
       "0    23179  0.892667  0.892471  0.894529  0.892329  1403  1275  210  112  \n",
       "1    17645  0.879000  0.878840  0.880180  0.878725  1373  1264  221  142  \n",
       "2    18669  0.877333  0.877246  0.877826  0.877154  1356  1276  209  159  \n",
       "3    15636  0.856333  0.856189  0.857082  0.856102  1332  1237  248  183  \n",
       "4    23296  0.891000  0.890845  0.892348  0.890712  1393  1280  205  122  \n",
       "5     1226  0.847333  0.846951  0.849716  0.846918  1346  1196  289  169  \n",
       "6     2611  0.863333  0.863169  0.864319  0.863073  1347  1243  242  168  \n",
       "7    17565  0.873667  0.873500  0.874825  0.873391  1365  1256  229  150  \n",
       "8    18607  0.882000  0.881869  0.882946  0.881755  1373  1273  212  142  \n",
       "9    15446  0.851667  0.851564  0.852094  0.851488  1317  1238  247  198  \n",
       "10   23107  0.891333  0.891141  0.893116  0.891002  1400  1274  211  115  \n",
       "11    1106  0.839333  0.838898  0.841884  0.838897  1337  1181  304  178  \n",
       "12    2473  0.861000  0.860823  0.862055  0.860729  1345  1238  247  170  \n",
       "13    9912  0.845333  0.845223  0.845766  0.845151  1308  1228  257  207  \n",
       "14   17560  0.878000  0.877853  0.879038  0.877741  1369  1265  220  146  \n",
       "15    1350  0.840667  0.840288  0.842845  0.840264  1334  1188  297  181  \n",
       "16    2524  0.859000  0.858840  0.859896  0.858749  1339  1238  247  176  \n",
       "17   10936  0.842000  0.841932  0.842179  0.841878  1294  1232  253  221  \n",
       "18   18621  0.878000  0.877904  0.878571  0.877808  1359  1275  210  156  \n",
       "19    1244  0.835333  0.835014  0.836994  0.834977  1319  1187  298  196  \n",
       "20    2427  0.854000  0.853885  0.854520  0.853805  1323  1239  246  192  \n",
       "21   15563  0.856000  0.855846  0.856816  0.855759  1333  1235  250  182  \n",
       "22    1220  0.800000  0.799812  0.800535  0.799773  1246  1154  331  269  \n",
       "23    2589  0.821667  0.821530  0.822102  0.821472  1274  1191  294  241  \n",
       "24    1226  0.846333  0.845953  0.848669  0.845921  1344  1195  290  171  \n",
       "25    2607  0.864000  0.863836  0.864988  0.863740  1348  1244  241  167  \n",
       "26    9832  0.846667  0.846597  0.846873  0.846538  1302  1238  247  213  \n",
       "27   17481  0.874333  0.874179  0.875385  0.874071  1364  1259  226  151  \n",
       "28    1283  0.831000  0.830647  0.832795  0.830626  1315  1178  307  200  \n",
       "29    2451  0.851333  0.851161  0.852228  0.851078  1328  1226  259  187  \n",
       "..     ...       ...       ...       ...       ...   ...   ...  ...  ...  \n",
       "822   2470  0.777000  0.776985  0.777129  0.777028  1153  1178  318  351  \n",
       "823   1364  0.817000  0.816997  0.817056  0.817017  1219  1232  264  285  \n",
       "824   2495  0.823000  0.822991  0.823112  0.823025  1224  1245  251  280  \n",
       "825  10821  0.822333  0.822234  0.823208  0.822402  1198  1269  227  306  \n",
       "826   1260  0.771000  0.770886  0.771672  0.771065  1123  1190  306  381  \n",
       "827   2383  0.753000  0.752988  0.753093  0.753024  1119  1140  356  385  \n",
       "828   1261  0.817000  0.816995  0.817068  0.817019  1218  1233  263  286  \n",
       "829   2396  0.826333  0.826319  0.826500  0.826363  1226  1253  243  278  \n",
       "830   1207  0.782333  0.782293  0.782628  0.782375  1153  1194  302  351  \n",
       "831   2595  0.764000  0.763998  0.764028  0.764013  1142  1150  346  362  \n",
       "832   9688  0.818333  0.818180  0.819590  0.818416  1184  1271  225  320  \n",
       "833   1296  0.764000  0.763918  0.764476  0.764055  1118  1174  322  386  \n",
       "834   2396  0.770333  0.770272  0.770718  0.770382  1131  1180  316  373  \n",
       "835   1298  0.811000  0.810997  0.811044  0.811016  1211  1222  274  293  \n",
       "836   2422  0.819000  0.818991  0.819111  0.819024  1218  1239  257  286  \n",
       "837  10757  0.824000  0.823796  0.825721  0.824097  1185  1287  209  319  \n",
       "838   1206  0.767000  0.766813  0.768023  0.767081  1108  1193  303  396  \n",
       "839   2327  0.748333  0.748319  0.748356  0.748318  1134  1111  385  370  \n",
       "840   1206  0.817667  0.817662  0.817735  0.817686  1219  1234  262  285  \n",
       "841   2340  0.809333  0.809309  0.809566  0.809369  1197  1231  265  307  \n",
       "842   1089  0.769333  0.768976  0.771236  0.769444  1095  1213  283  409  \n",
       "843   2454  0.756000  0.755982  0.756129  0.756028  1121  1147  349  383  \n",
       "844   1358  0.774000  0.773977  0.774171  0.774032  1146  1176  320  358  \n",
       "845   2471  0.775667  0.775625  0.775955  0.775709  1143  1184  312  361  \n",
       "846   1255  0.770667  0.770549  0.771356  0.770733  1122  1190  306  382  \n",
       "847   2372  0.760333  0.760329  0.760382  0.760350  1134  1147  349  370  \n",
       "848   1292  0.763667  0.763581  0.764158  0.763723  1117  1174  322  387  \n",
       "849   2398  0.769667  0.769595  0.770107  0.769719  1128  1181  315  376  \n",
       "850   1200  0.767000  0.766785  0.768160  0.767086  1105  1196  300  399  \n",
       "851   2316  0.755000  0.754994  0.755004  0.754992  1140  1125  371  364  \n",
       "\n",
       "[852 rows x 11 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.columns = ['clf_name','comb','features_number','accuracy','f1_score','precision','recall','true_positives','true_negatives','false_positives','false_negatives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf_name</th>\n",
       "      <th>comb</th>\n",
       "      <th>features_number</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>true_positives</th>\n",
       "      <th>true_negatives</th>\n",
       "      <th>false_positives</th>\n",
       "      <th>false_negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words',)</td>\n",
       "      <td>23179</td>\n",
       "      <td>0.892667</td>\n",
       "      <td>0.892471</td>\n",
       "      <td>0.894529</td>\n",
       "      <td>0.892329</td>\n",
       "      <td>1403</td>\n",
       "      <td>1275</td>\n",
       "      <td>210</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stemming',)</td>\n",
       "      <td>17645</td>\n",
       "      <td>0.879000</td>\n",
       "      <td>0.878840</td>\n",
       "      <td>0.880180</td>\n",
       "      <td>0.878725</td>\n",
       "      <td>1373</td>\n",
       "      <td>1264</td>\n",
       "      <td>221</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('lemmatization',)</td>\n",
       "      <td>18669</td>\n",
       "      <td>0.877333</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.877826</td>\n",
       "      <td>0.877154</td>\n",
       "      <td>1356</td>\n",
       "      <td>1276</td>\n",
       "      <td>209</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('urls',)</td>\n",
       "      <td>15636</td>\n",
       "      <td>0.856333</td>\n",
       "      <td>0.856189</td>\n",
       "      <td>0.857082</td>\n",
       "      <td>0.856102</td>\n",
       "      <td>1332</td>\n",
       "      <td>1237</td>\n",
       "      <td>248</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('norm_letters',)</td>\n",
       "      <td>23296</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.892348</td>\n",
       "      <td>0.890712</td>\n",
       "      <td>1393</td>\n",
       "      <td>1280</td>\n",
       "      <td>205</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('pruning10',)</td>\n",
       "      <td>1226</td>\n",
       "      <td>0.847333</td>\n",
       "      <td>0.846951</td>\n",
       "      <td>0.849716</td>\n",
       "      <td>0.846918</td>\n",
       "      <td>1346</td>\n",
       "      <td>1196</td>\n",
       "      <td>289</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('pruning5',)</td>\n",
       "      <td>2611</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.863169</td>\n",
       "      <td>0.864319</td>\n",
       "      <td>0.863073</td>\n",
       "      <td>1347</td>\n",
       "      <td>1243</td>\n",
       "      <td>242</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'stemming')</td>\n",
       "      <td>17565</td>\n",
       "      <td>0.873667</td>\n",
       "      <td>0.873500</td>\n",
       "      <td>0.874825</td>\n",
       "      <td>0.873391</td>\n",
       "      <td>1365</td>\n",
       "      <td>1256</td>\n",
       "      <td>229</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'lemmatization')</td>\n",
       "      <td>18607</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>0.881869</td>\n",
       "      <td>0.882946</td>\n",
       "      <td>0.881755</td>\n",
       "      <td>1373</td>\n",
       "      <td>1273</td>\n",
       "      <td>212</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'urls')</td>\n",
       "      <td>15446</td>\n",
       "      <td>0.851667</td>\n",
       "      <td>0.851564</td>\n",
       "      <td>0.852094</td>\n",
       "      <td>0.851488</td>\n",
       "      <td>1317</td>\n",
       "      <td>1238</td>\n",
       "      <td>247</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'norm_letters')</td>\n",
       "      <td>23107</td>\n",
       "      <td>0.891333</td>\n",
       "      <td>0.891141</td>\n",
       "      <td>0.893116</td>\n",
       "      <td>0.891002</td>\n",
       "      <td>1400</td>\n",
       "      <td>1274</td>\n",
       "      <td>211</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'pruning10')</td>\n",
       "      <td>1106</td>\n",
       "      <td>0.839333</td>\n",
       "      <td>0.838898</td>\n",
       "      <td>0.841884</td>\n",
       "      <td>0.838897</td>\n",
       "      <td>1337</td>\n",
       "      <td>1181</td>\n",
       "      <td>304</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'pruning5')</td>\n",
       "      <td>2473</td>\n",
       "      <td>0.861000</td>\n",
       "      <td>0.860823</td>\n",
       "      <td>0.862055</td>\n",
       "      <td>0.860729</td>\n",
       "      <td>1345</td>\n",
       "      <td>1238</td>\n",
       "      <td>247</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stemming', 'urls')</td>\n",
       "      <td>9912</td>\n",
       "      <td>0.845333</td>\n",
       "      <td>0.845223</td>\n",
       "      <td>0.845766</td>\n",
       "      <td>0.845151</td>\n",
       "      <td>1308</td>\n",
       "      <td>1228</td>\n",
       "      <td>257</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stemming', 'norm_letters')</td>\n",
       "      <td>17560</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>0.877853</td>\n",
       "      <td>0.879038</td>\n",
       "      <td>0.877741</td>\n",
       "      <td>1369</td>\n",
       "      <td>1265</td>\n",
       "      <td>220</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stemming', 'pruning10')</td>\n",
       "      <td>1350</td>\n",
       "      <td>0.840667</td>\n",
       "      <td>0.840288</td>\n",
       "      <td>0.842845</td>\n",
       "      <td>0.840264</td>\n",
       "      <td>1334</td>\n",
       "      <td>1188</td>\n",
       "      <td>297</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stemming', 'pruning5')</td>\n",
       "      <td>2524</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.858840</td>\n",
       "      <td>0.859896</td>\n",
       "      <td>0.858749</td>\n",
       "      <td>1339</td>\n",
       "      <td>1238</td>\n",
       "      <td>247</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('lemmatization', 'urls')</td>\n",
       "      <td>10936</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.841932</td>\n",
       "      <td>0.842179</td>\n",
       "      <td>0.841878</td>\n",
       "      <td>1294</td>\n",
       "      <td>1232</td>\n",
       "      <td>253</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('lemmatization', 'norm_letters')</td>\n",
       "      <td>18621</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>0.877904</td>\n",
       "      <td>0.878571</td>\n",
       "      <td>0.877808</td>\n",
       "      <td>1359</td>\n",
       "      <td>1275</td>\n",
       "      <td>210</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('lemmatization', 'pruning10')</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.835333</td>\n",
       "      <td>0.835014</td>\n",
       "      <td>0.836994</td>\n",
       "      <td>0.834977</td>\n",
       "      <td>1319</td>\n",
       "      <td>1187</td>\n",
       "      <td>298</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('lemmatization', 'pruning5')</td>\n",
       "      <td>2427</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.853885</td>\n",
       "      <td>0.854520</td>\n",
       "      <td>0.853805</td>\n",
       "      <td>1323</td>\n",
       "      <td>1239</td>\n",
       "      <td>246</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('urls', 'norm_letters')</td>\n",
       "      <td>15563</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.855846</td>\n",
       "      <td>0.856816</td>\n",
       "      <td>0.855759</td>\n",
       "      <td>1333</td>\n",
       "      <td>1235</td>\n",
       "      <td>250</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('urls', 'pruning10')</td>\n",
       "      <td>1220</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.799812</td>\n",
       "      <td>0.800535</td>\n",
       "      <td>0.799773</td>\n",
       "      <td>1246</td>\n",
       "      <td>1154</td>\n",
       "      <td>331</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('urls', 'pruning5')</td>\n",
       "      <td>2589</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.821530</td>\n",
       "      <td>0.822102</td>\n",
       "      <td>0.821472</td>\n",
       "      <td>1274</td>\n",
       "      <td>1191</td>\n",
       "      <td>294</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('norm_letters', 'pruning10')</td>\n",
       "      <td>1226</td>\n",
       "      <td>0.846333</td>\n",
       "      <td>0.845953</td>\n",
       "      <td>0.848669</td>\n",
       "      <td>0.845921</td>\n",
       "      <td>1344</td>\n",
       "      <td>1195</td>\n",
       "      <td>290</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('norm_letters', 'pruning5')</td>\n",
       "      <td>2607</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.863836</td>\n",
       "      <td>0.864988</td>\n",
       "      <td>0.863740</td>\n",
       "      <td>1348</td>\n",
       "      <td>1244</td>\n",
       "      <td>241</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'stemming', 'urls')</td>\n",
       "      <td>9832</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.846597</td>\n",
       "      <td>0.846873</td>\n",
       "      <td>0.846538</td>\n",
       "      <td>1302</td>\n",
       "      <td>1238</td>\n",
       "      <td>247</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'stemming', 'norm_letters')</td>\n",
       "      <td>17481</td>\n",
       "      <td>0.874333</td>\n",
       "      <td>0.874179</td>\n",
       "      <td>0.875385</td>\n",
       "      <td>0.874071</td>\n",
       "      <td>1364</td>\n",
       "      <td>1259</td>\n",
       "      <td>226</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'stemming', 'pruning10')</td>\n",
       "      <td>1283</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>0.830647</td>\n",
       "      <td>0.832795</td>\n",
       "      <td>0.830626</td>\n",
       "      <td>1315</td>\n",
       "      <td>1178</td>\n",
       "      <td>307</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Bayesiano_TF</td>\n",
       "      <td>('stop_words', 'stemming', 'pruning5')</td>\n",
       "      <td>2451</td>\n",
       "      <td>0.851333</td>\n",
       "      <td>0.851161</td>\n",
       "      <td>0.852228</td>\n",
       "      <td>0.851078</td>\n",
       "      <td>1328</td>\n",
       "      <td>1226</td>\n",
       "      <td>259</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stemming', 'urls', 'pruning5')</td>\n",
       "      <td>2470</td>\n",
       "      <td>0.777000</td>\n",
       "      <td>0.776985</td>\n",
       "      <td>0.777129</td>\n",
       "      <td>0.777028</td>\n",
       "      <td>1153</td>\n",
       "      <td>1178</td>\n",
       "      <td>318</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stemming', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1364</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.816997</td>\n",
       "      <td>0.817056</td>\n",
       "      <td>0.817017</td>\n",
       "      <td>1219</td>\n",
       "      <td>1232</td>\n",
       "      <td>264</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stemming', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2495</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.822991</td>\n",
       "      <td>0.823112</td>\n",
       "      <td>0.823025</td>\n",
       "      <td>1224</td>\n",
       "      <td>1245</td>\n",
       "      <td>251</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'urls', 'norm_letters')</td>\n",
       "      <td>10821</td>\n",
       "      <td>0.822333</td>\n",
       "      <td>0.822234</td>\n",
       "      <td>0.823208</td>\n",
       "      <td>0.822402</td>\n",
       "      <td>1198</td>\n",
       "      <td>1269</td>\n",
       "      <td>227</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'urls', 'pruning10')</td>\n",
       "      <td>1260</td>\n",
       "      <td>0.771000</td>\n",
       "      <td>0.770886</td>\n",
       "      <td>0.771672</td>\n",
       "      <td>0.771065</td>\n",
       "      <td>1123</td>\n",
       "      <td>1190</td>\n",
       "      <td>306</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'urls', 'pruning5')</td>\n",
       "      <td>2383</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.752988</td>\n",
       "      <td>0.753093</td>\n",
       "      <td>0.753024</td>\n",
       "      <td>1119</td>\n",
       "      <td>1140</td>\n",
       "      <td>356</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1261</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.816995</td>\n",
       "      <td>0.817068</td>\n",
       "      <td>0.817019</td>\n",
       "      <td>1218</td>\n",
       "      <td>1233</td>\n",
       "      <td>263</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2396</td>\n",
       "      <td>0.826333</td>\n",
       "      <td>0.826319</td>\n",
       "      <td>0.826500</td>\n",
       "      <td>0.826363</td>\n",
       "      <td>1226</td>\n",
       "      <td>1253</td>\n",
       "      <td>243</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('urls', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1207</td>\n",
       "      <td>0.782333</td>\n",
       "      <td>0.782293</td>\n",
       "      <td>0.782628</td>\n",
       "      <td>0.782375</td>\n",
       "      <td>1153</td>\n",
       "      <td>1194</td>\n",
       "      <td>302</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2595</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>0.763998</td>\n",
       "      <td>0.764028</td>\n",
       "      <td>0.764013</td>\n",
       "      <td>1142</td>\n",
       "      <td>1150</td>\n",
       "      <td>346</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'urls', 'norm_letters')</td>\n",
       "      <td>9688</td>\n",
       "      <td>0.818333</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>0.819590</td>\n",
       "      <td>0.818416</td>\n",
       "      <td>1184</td>\n",
       "      <td>1271</td>\n",
       "      <td>225</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'urls', 'pruning10')</td>\n",
       "      <td>1296</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>0.763918</td>\n",
       "      <td>0.764476</td>\n",
       "      <td>0.764055</td>\n",
       "      <td>1118</td>\n",
       "      <td>1174</td>\n",
       "      <td>322</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'urls', 'pruning5')</td>\n",
       "      <td>2396</td>\n",
       "      <td>0.770333</td>\n",
       "      <td>0.770272</td>\n",
       "      <td>0.770718</td>\n",
       "      <td>0.770382</td>\n",
       "      <td>1131</td>\n",
       "      <td>1180</td>\n",
       "      <td>316</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1298</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.810997</td>\n",
       "      <td>0.811044</td>\n",
       "      <td>0.811016</td>\n",
       "      <td>1211</td>\n",
       "      <td>1222</td>\n",
       "      <td>274</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2422</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.818991</td>\n",
       "      <td>0.819111</td>\n",
       "      <td>0.819024</td>\n",
       "      <td>1218</td>\n",
       "      <td>1239</td>\n",
       "      <td>257</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'urls', 'norm_letters')</td>\n",
       "      <td>10757</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.823796</td>\n",
       "      <td>0.825721</td>\n",
       "      <td>0.824097</td>\n",
       "      <td>1185</td>\n",
       "      <td>1287</td>\n",
       "      <td>209</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'urls', 'pruning10')</td>\n",
       "      <td>1206</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.766813</td>\n",
       "      <td>0.768023</td>\n",
       "      <td>0.767081</td>\n",
       "      <td>1108</td>\n",
       "      <td>1193</td>\n",
       "      <td>303</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'urls', 'pruning5')</td>\n",
       "      <td>2327</td>\n",
       "      <td>0.748333</td>\n",
       "      <td>0.748319</td>\n",
       "      <td>0.748356</td>\n",
       "      <td>0.748318</td>\n",
       "      <td>1134</td>\n",
       "      <td>1111</td>\n",
       "      <td>385</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1206</td>\n",
       "      <td>0.817667</td>\n",
       "      <td>0.817662</td>\n",
       "      <td>0.817735</td>\n",
       "      <td>0.817686</td>\n",
       "      <td>1219</td>\n",
       "      <td>1234</td>\n",
       "      <td>262</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2340</td>\n",
       "      <td>0.809333</td>\n",
       "      <td>0.809309</td>\n",
       "      <td>0.809566</td>\n",
       "      <td>0.809369</td>\n",
       "      <td>1197</td>\n",
       "      <td>1231</td>\n",
       "      <td>265</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'urls', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1089</td>\n",
       "      <td>0.769333</td>\n",
       "      <td>0.768976</td>\n",
       "      <td>0.771236</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>1095</td>\n",
       "      <td>1213</td>\n",
       "      <td>283</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2454</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.755982</td>\n",
       "      <td>0.756129</td>\n",
       "      <td>0.756028</td>\n",
       "      <td>1121</td>\n",
       "      <td>1147</td>\n",
       "      <td>349</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stemming', 'urls', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1358</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.773977</td>\n",
       "      <td>0.774171</td>\n",
       "      <td>0.774032</td>\n",
       "      <td>1146</td>\n",
       "      <td>1176</td>\n",
       "      <td>320</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stemming', 'urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2471</td>\n",
       "      <td>0.775667</td>\n",
       "      <td>0.775625</td>\n",
       "      <td>0.775955</td>\n",
       "      <td>0.775709</td>\n",
       "      <td>1143</td>\n",
       "      <td>1184</td>\n",
       "      <td>312</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'urls', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1255</td>\n",
       "      <td>0.770667</td>\n",
       "      <td>0.770549</td>\n",
       "      <td>0.771356</td>\n",
       "      <td>0.770733</td>\n",
       "      <td>1122</td>\n",
       "      <td>1190</td>\n",
       "      <td>306</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2372</td>\n",
       "      <td>0.760333</td>\n",
       "      <td>0.760329</td>\n",
       "      <td>0.760382</td>\n",
       "      <td>0.760350</td>\n",
       "      <td>1134</td>\n",
       "      <td>1147</td>\n",
       "      <td>349</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1292</td>\n",
       "      <td>0.763667</td>\n",
       "      <td>0.763581</td>\n",
       "      <td>0.764158</td>\n",
       "      <td>0.763723</td>\n",
       "      <td>1117</td>\n",
       "      <td>1174</td>\n",
       "      <td>322</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2398</td>\n",
       "      <td>0.769667</td>\n",
       "      <td>0.769595</td>\n",
       "      <td>0.770107</td>\n",
       "      <td>0.769719</td>\n",
       "      <td>1128</td>\n",
       "      <td>1181</td>\n",
       "      <td>315</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning10')</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.766785</td>\n",
       "      <td>0.768160</td>\n",
       "      <td>0.767086</td>\n",
       "      <td>1105</td>\n",
       "      <td>1196</td>\n",
       "      <td>300</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2316</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.754994</td>\n",
       "      <td>0.755004</td>\n",
       "      <td>0.754992</td>\n",
       "      <td>1140</td>\n",
       "      <td>1125</td>\n",
       "      <td>371</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>852 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            clf_name  \\\n",
       "0       Bayesiano_TF   \n",
       "1       Bayesiano_TF   \n",
       "2       Bayesiano_TF   \n",
       "3       Bayesiano_TF   \n",
       "4       Bayesiano_TF   \n",
       "5       Bayesiano_TF   \n",
       "6       Bayesiano_TF   \n",
       "7       Bayesiano_TF   \n",
       "8       Bayesiano_TF   \n",
       "9       Bayesiano_TF   \n",
       "10      Bayesiano_TF   \n",
       "11      Bayesiano_TF   \n",
       "12      Bayesiano_TF   \n",
       "13      Bayesiano_TF   \n",
       "14      Bayesiano_TF   \n",
       "15      Bayesiano_TF   \n",
       "16      Bayesiano_TF   \n",
       "17      Bayesiano_TF   \n",
       "18      Bayesiano_TF   \n",
       "19      Bayesiano_TF   \n",
       "20      Bayesiano_TF   \n",
       "21      Bayesiano_TF   \n",
       "22      Bayesiano_TF   \n",
       "23      Bayesiano_TF   \n",
       "24      Bayesiano_TF   \n",
       "25      Bayesiano_TF   \n",
       "26      Bayesiano_TF   \n",
       "27      Bayesiano_TF   \n",
       "28      Bayesiano_TF   \n",
       "29      Bayesiano_TF   \n",
       "..               ...   \n",
       "822  Reg_Log_Binario   \n",
       "823  Reg_Log_Binario   \n",
       "824  Reg_Log_Binario   \n",
       "825  Reg_Log_Binario   \n",
       "826  Reg_Log_Binario   \n",
       "827  Reg_Log_Binario   \n",
       "828  Reg_Log_Binario   \n",
       "829  Reg_Log_Binario   \n",
       "830  Reg_Log_Binario   \n",
       "831  Reg_Log_Binario   \n",
       "832  Reg_Log_Binario   \n",
       "833  Reg_Log_Binario   \n",
       "834  Reg_Log_Binario   \n",
       "835  Reg_Log_Binario   \n",
       "836  Reg_Log_Binario   \n",
       "837  Reg_Log_Binario   \n",
       "838  Reg_Log_Binario   \n",
       "839  Reg_Log_Binario   \n",
       "840  Reg_Log_Binario   \n",
       "841  Reg_Log_Binario   \n",
       "842  Reg_Log_Binario   \n",
       "843  Reg_Log_Binario   \n",
       "844  Reg_Log_Binario   \n",
       "845  Reg_Log_Binario   \n",
       "846  Reg_Log_Binario   \n",
       "847  Reg_Log_Binario   \n",
       "848  Reg_Log_Binario   \n",
       "849  Reg_Log_Binario   \n",
       "850  Reg_Log_Binario   \n",
       "851  Reg_Log_Binario   \n",
       "\n",
       "                                                                     comb  \\\n",
       "0                                                         ('stop_words',)   \n",
       "1                                                           ('stemming',)   \n",
       "2                                                      ('lemmatization',)   \n",
       "3                                                               ('urls',)   \n",
       "4                                                       ('norm_letters',)   \n",
       "5                                                          ('pruning10',)   \n",
       "6                                                           ('pruning5',)   \n",
       "7                                              ('stop_words', 'stemming')   \n",
       "8                                         ('stop_words', 'lemmatization')   \n",
       "9                                                  ('stop_words', 'urls')   \n",
       "10                                         ('stop_words', 'norm_letters')   \n",
       "11                                            ('stop_words', 'pruning10')   \n",
       "12                                             ('stop_words', 'pruning5')   \n",
       "13                                                   ('stemming', 'urls')   \n",
       "14                                           ('stemming', 'norm_letters')   \n",
       "15                                              ('stemming', 'pruning10')   \n",
       "16                                               ('stemming', 'pruning5')   \n",
       "17                                              ('lemmatization', 'urls')   \n",
       "18                                      ('lemmatization', 'norm_letters')   \n",
       "19                                         ('lemmatization', 'pruning10')   \n",
       "20                                          ('lemmatization', 'pruning5')   \n",
       "21                                               ('urls', 'norm_letters')   \n",
       "22                                                  ('urls', 'pruning10')   \n",
       "23                                                   ('urls', 'pruning5')   \n",
       "24                                          ('norm_letters', 'pruning10')   \n",
       "25                                           ('norm_letters', 'pruning5')   \n",
       "26                                     ('stop_words', 'stemming', 'urls')   \n",
       "27                             ('stop_words', 'stemming', 'norm_letters')   \n",
       "28                                ('stop_words', 'stemming', 'pruning10')   \n",
       "29                                 ('stop_words', 'stemming', 'pruning5')   \n",
       "..                                                                    ...   \n",
       "822                                      ('stemming', 'urls', 'pruning5')   \n",
       "823                             ('stemming', 'norm_letters', 'pruning10')   \n",
       "824                              ('stemming', 'norm_letters', 'pruning5')   \n",
       "825                             ('lemmatization', 'urls', 'norm_letters')   \n",
       "826                                ('lemmatization', 'urls', 'pruning10')   \n",
       "827                                 ('lemmatization', 'urls', 'pruning5')   \n",
       "828                        ('lemmatization', 'norm_letters', 'pruning10')   \n",
       "829                         ('lemmatization', 'norm_letters', 'pruning5')   \n",
       "830                                 ('urls', 'norm_letters', 'pruning10')   \n",
       "831                                  ('urls', 'norm_letters', 'pruning5')   \n",
       "832                    ('stop_words', 'stemming', 'urls', 'norm_letters')   \n",
       "833                       ('stop_words', 'stemming', 'urls', 'pruning10')   \n",
       "834                        ('stop_words', 'stemming', 'urls', 'pruning5')   \n",
       "835               ('stop_words', 'stemming', 'norm_letters', 'pruning10')   \n",
       "836                ('stop_words', 'stemming', 'norm_letters', 'pruning5')   \n",
       "837               ('stop_words', 'lemmatization', 'urls', 'norm_letters')   \n",
       "838                  ('stop_words', 'lemmatization', 'urls', 'pruning10')   \n",
       "839                   ('stop_words', 'lemmatization', 'urls', 'pruning5')   \n",
       "840          ('stop_words', 'lemmatization', 'norm_letters', 'pruning10')   \n",
       "841           ('stop_words', 'lemmatization', 'norm_letters', 'pruning5')   \n",
       "842                   ('stop_words', 'urls', 'norm_letters', 'pruning10')   \n",
       "843                    ('stop_words', 'urls', 'norm_letters', 'pruning5')   \n",
       "844                     ('stemming', 'urls', 'norm_letters', 'pruning10')   \n",
       "845                      ('stemming', 'urls', 'norm_letters', 'pruning5')   \n",
       "846                ('lemmatization', 'urls', 'norm_letters', 'pruning10')   \n",
       "847                 ('lemmatization', 'urls', 'norm_letters', 'pruning5')   \n",
       "848       ('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning10')   \n",
       "849        ('stop_words', 'stemming', 'urls', 'norm_letters', 'pruning5')   \n",
       "850  ('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning10')   \n",
       "851   ('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning5')   \n",
       "\n",
       "     features_number  accuracy  f1_score  precision    recall  true_positives  \\\n",
       "0              23179  0.892667  0.892471   0.894529  0.892329            1403   \n",
       "1              17645  0.879000  0.878840   0.880180  0.878725            1373   \n",
       "2              18669  0.877333  0.877246   0.877826  0.877154            1356   \n",
       "3              15636  0.856333  0.856189   0.857082  0.856102            1332   \n",
       "4              23296  0.891000  0.890845   0.892348  0.890712            1393   \n",
       "5               1226  0.847333  0.846951   0.849716  0.846918            1346   \n",
       "6               2611  0.863333  0.863169   0.864319  0.863073            1347   \n",
       "7              17565  0.873667  0.873500   0.874825  0.873391            1365   \n",
       "8              18607  0.882000  0.881869   0.882946  0.881755            1373   \n",
       "9              15446  0.851667  0.851564   0.852094  0.851488            1317   \n",
       "10             23107  0.891333  0.891141   0.893116  0.891002            1400   \n",
       "11              1106  0.839333  0.838898   0.841884  0.838897            1337   \n",
       "12              2473  0.861000  0.860823   0.862055  0.860729            1345   \n",
       "13              9912  0.845333  0.845223   0.845766  0.845151            1308   \n",
       "14             17560  0.878000  0.877853   0.879038  0.877741            1369   \n",
       "15              1350  0.840667  0.840288   0.842845  0.840264            1334   \n",
       "16              2524  0.859000  0.858840   0.859896  0.858749            1339   \n",
       "17             10936  0.842000  0.841932   0.842179  0.841878            1294   \n",
       "18             18621  0.878000  0.877904   0.878571  0.877808            1359   \n",
       "19              1244  0.835333  0.835014   0.836994  0.834977            1319   \n",
       "20              2427  0.854000  0.853885   0.854520  0.853805            1323   \n",
       "21             15563  0.856000  0.855846   0.856816  0.855759            1333   \n",
       "22              1220  0.800000  0.799812   0.800535  0.799773            1246   \n",
       "23              2589  0.821667  0.821530   0.822102  0.821472            1274   \n",
       "24              1226  0.846333  0.845953   0.848669  0.845921            1344   \n",
       "25              2607  0.864000  0.863836   0.864988  0.863740            1348   \n",
       "26              9832  0.846667  0.846597   0.846873  0.846538            1302   \n",
       "27             17481  0.874333  0.874179   0.875385  0.874071            1364   \n",
       "28              1283  0.831000  0.830647   0.832795  0.830626            1315   \n",
       "29              2451  0.851333  0.851161   0.852228  0.851078            1328   \n",
       "..               ...       ...       ...        ...       ...             ...   \n",
       "822             2470  0.777000  0.776985   0.777129  0.777028            1153   \n",
       "823             1364  0.817000  0.816997   0.817056  0.817017            1219   \n",
       "824             2495  0.823000  0.822991   0.823112  0.823025            1224   \n",
       "825            10821  0.822333  0.822234   0.823208  0.822402            1198   \n",
       "826             1260  0.771000  0.770886   0.771672  0.771065            1123   \n",
       "827             2383  0.753000  0.752988   0.753093  0.753024            1119   \n",
       "828             1261  0.817000  0.816995   0.817068  0.817019            1218   \n",
       "829             2396  0.826333  0.826319   0.826500  0.826363            1226   \n",
       "830             1207  0.782333  0.782293   0.782628  0.782375            1153   \n",
       "831             2595  0.764000  0.763998   0.764028  0.764013            1142   \n",
       "832             9688  0.818333  0.818180   0.819590  0.818416            1184   \n",
       "833             1296  0.764000  0.763918   0.764476  0.764055            1118   \n",
       "834             2396  0.770333  0.770272   0.770718  0.770382            1131   \n",
       "835             1298  0.811000  0.810997   0.811044  0.811016            1211   \n",
       "836             2422  0.819000  0.818991   0.819111  0.819024            1218   \n",
       "837            10757  0.824000  0.823796   0.825721  0.824097            1185   \n",
       "838             1206  0.767000  0.766813   0.768023  0.767081            1108   \n",
       "839             2327  0.748333  0.748319   0.748356  0.748318            1134   \n",
       "840             1206  0.817667  0.817662   0.817735  0.817686            1219   \n",
       "841             2340  0.809333  0.809309   0.809566  0.809369            1197   \n",
       "842             1089  0.769333  0.768976   0.771236  0.769444            1095   \n",
       "843             2454  0.756000  0.755982   0.756129  0.756028            1121   \n",
       "844             1358  0.774000  0.773977   0.774171  0.774032            1146   \n",
       "845             2471  0.775667  0.775625   0.775955  0.775709            1143   \n",
       "846             1255  0.770667  0.770549   0.771356  0.770733            1122   \n",
       "847             2372  0.760333  0.760329   0.760382  0.760350            1134   \n",
       "848             1292  0.763667  0.763581   0.764158  0.763723            1117   \n",
       "849             2398  0.769667  0.769595   0.770107  0.769719            1128   \n",
       "850             1200  0.767000  0.766785   0.768160  0.767086            1105   \n",
       "851             2316  0.755000  0.754994   0.755004  0.754992            1140   \n",
       "\n",
       "     true_negatives  false_positives  false_negatives  \n",
       "0              1275              210              112  \n",
       "1              1264              221              142  \n",
       "2              1276              209              159  \n",
       "3              1237              248              183  \n",
       "4              1280              205              122  \n",
       "5              1196              289              169  \n",
       "6              1243              242              168  \n",
       "7              1256              229              150  \n",
       "8              1273              212              142  \n",
       "9              1238              247              198  \n",
       "10             1274              211              115  \n",
       "11             1181              304              178  \n",
       "12             1238              247              170  \n",
       "13             1228              257              207  \n",
       "14             1265              220              146  \n",
       "15             1188              297              181  \n",
       "16             1238              247              176  \n",
       "17             1232              253              221  \n",
       "18             1275              210              156  \n",
       "19             1187              298              196  \n",
       "20             1239              246              192  \n",
       "21             1235              250              182  \n",
       "22             1154              331              269  \n",
       "23             1191              294              241  \n",
       "24             1195              290              171  \n",
       "25             1244              241              167  \n",
       "26             1238              247              213  \n",
       "27             1259              226              151  \n",
       "28             1178              307              200  \n",
       "29             1226              259              187  \n",
       "..              ...              ...              ...  \n",
       "822            1178              318              351  \n",
       "823            1232              264              285  \n",
       "824            1245              251              280  \n",
       "825            1269              227              306  \n",
       "826            1190              306              381  \n",
       "827            1140              356              385  \n",
       "828            1233              263              286  \n",
       "829            1253              243              278  \n",
       "830            1194              302              351  \n",
       "831            1150              346              362  \n",
       "832            1271              225              320  \n",
       "833            1174              322              386  \n",
       "834            1180              316              373  \n",
       "835            1222              274              293  \n",
       "836            1239              257              286  \n",
       "837            1287              209              319  \n",
       "838            1193              303              396  \n",
       "839            1111              385              370  \n",
       "840            1234              262              285  \n",
       "841            1231              265              307  \n",
       "842            1213              283              409  \n",
       "843            1147              349              383  \n",
       "844            1176              320              358  \n",
       "845            1184              312              361  \n",
       "846            1190              306              382  \n",
       "847            1147              349              370  \n",
       "848            1174              322              387  \n",
       "849            1181              315              376  \n",
       "850            1196              300              399  \n",
       "851            1125              371              364  \n",
       "\n",
       "[852 rows x 11 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('result_news_fakes_combinations.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tweets_df = results_df[results_df['clf_name'].str.contains('Reg_Log')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_result = results_tweets_df.nsmallest(5, columns='accuracy')[['clf_name','comb','features_number','accuracy','f1_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf_name</th>\n",
       "      <th>comb</th>\n",
       "      <th>features_number</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'urls', 'pruning5')</td>\n",
       "      <td>2327</td>\n",
       "      <td>0.748333</td>\n",
       "      <td>0.748319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('lemmatization', 'urls', 'pruning5')</td>\n",
       "      <td>2383</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.752988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2316</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.754994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'urls', 'norm_letters', 'pruning5')</td>\n",
       "      <td>2454</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.755982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>Reg_Log_Binario</td>\n",
       "      <td>('stop_words', 'urls', 'pruning5')</td>\n",
       "      <td>2457</td>\n",
       "      <td>0.757667</td>\n",
       "      <td>0.757657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            clf_name  \\\n",
       "839  Reg_Log_Binario   \n",
       "827  Reg_Log_Binario   \n",
       "851  Reg_Log_Binario   \n",
       "843  Reg_Log_Binario   \n",
       "817  Reg_Log_Binario   \n",
       "\n",
       "                                                                    comb  \\\n",
       "839                  ('stop_words', 'lemmatization', 'urls', 'pruning5')   \n",
       "827                                ('lemmatization', 'urls', 'pruning5')   \n",
       "851  ('stop_words', 'lemmatization', 'urls', 'norm_letters', 'pruning5')   \n",
       "843                   ('stop_words', 'urls', 'norm_letters', 'pruning5')   \n",
       "817                                   ('stop_words', 'urls', 'pruning5')   \n",
       "\n",
       "     features_number  accuracy  f1_score  \n",
       "839             2327  0.748333  0.748319  \n",
       "827             2383  0.753000  0.752988  \n",
       "851             2316  0.755000  0.754994  \n",
       "843             2454  0.756000  0.755982  \n",
       "817             2457  0.757667  0.757657  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_result.columns = ['Clf_Pon', 'Combinación', '# Car', 'Exact', 'V-F1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{|l|m{16em}|r|r|r|}\n",
      "\\toprule\n",
      "         Clf\\_Pon &                                                          Combinación &  \\# Car &   Exact &    V-F1 \\\\\n",
      "\\midrule\n",
      " Reg\\_Log\\_Binario &                  ('stop\\_words', 'lemmatization', 'urls', 'pruning5') &   2327 &  0,7483 &  0,7483 \\\\\n",
      " Reg\\_Log\\_Binario &                                ('lemmatization', 'urls', 'pruning5') &   2383 &  0,7530 &  0,7530 \\\\\n",
      " Reg\\_Log\\_Binario &  ('stop\\_words', 'lemmatization', 'urls', 'norm\\_letters', 'pruning5') &   2316 &  0,7550 &  0,7550 \\\\\n",
      " Reg\\_Log\\_Binario &                   ('stop\\_words', 'urls', 'norm\\_letters', 'pruning5') &   2454 &  0,7560 &  0,7560 \\\\\n",
      " Reg\\_Log\\_Binario &                                   ('stop\\_words', 'urls', 'pruning5') &   2457 &  0,7577 &  0,7577 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(filter_result.round(4).to_latex(index=False, column_format='|l|m{16em}|r|r|r|', decimal=','))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_tg",
   "language": "python",
   "name": "python_tg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
